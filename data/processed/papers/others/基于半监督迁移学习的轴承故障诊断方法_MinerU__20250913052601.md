摘要：针对航空发动机轴承故障诊断过程中预测精度不足以及过拟合的问题，提出基于迁移学习的半监督集成学习器（SSIT）用以发动机轴承故障预测。首先，训练改进的基于迁移学习的极限学习机（TELM）以及基于迁移学习的支持向量机（TSVM），分别迁移不同目标空间的高相似度样本加入到源样本空间进行训练。然后，与对应的基学习器集成同簇学习器来识别未标记样本，构成半监督学习器不断调整初始基学习器权重，并继续集成半监督基学习器的识别结果到SSIT中。通过此学习机识别提取特征后的，用以故障识别。实验结果清楚地表明：此种方法可以有效降低迁移学习中的负迁移效果，提升迁移精度  $10\%$  左右，降低机器学习中的过拟合效果，提高半监督学习稳定性，与现有的预测方法相比可以提高精度  $9\%$  以上。

关键词：航空发动机；故障诊断；半监督；迁移学习；集成学习  中图分类号：TH133；TP391  文献标识码：A 文章编号：1001- 5965（2019）11- 2291- 40

在航空发动机的故障预测中，由于其系统复杂，长时间处于高温度、高速度等困难环境下，不仅部件更易损坏，而且故障的预测也更为困难。现有的轴承故障预测方法大多选择在实验状态下测试，数据都在低负载、低速度以及常温常压下采集，对比发动机内环境过于温和，而在发动机内部的轴承振动数据又面临具体故障的已标记样本获取难度大以及过拟合等问题。

在基于神经网络的轴承故障预测领域中，机器学习一直面临着可用样本不足、过拟合等问题，大多数情况下，标签数据的获取成本很高，而未标记的数据则很容易获得而且数量更多。基于这个原因，半监督学习（Semi- Supervised Learning，SSL）获得了越来越多的关注，其致力于从未标记样本中获得信息，寻找大量的未标记数据中的规律，再利用少部分已标记数据的信息，做出预测。

大多数现有的半监督学习技术主要是在未标记数据的信息获取方式上加以区别。半监督学习方法中，少量的有标记样本其实是训练学习的核心，过拟合的效果很难消除，同时随着信息的增加，新增的未标记样本也未必严格与已标记样本同分布。

知识迁移就是一种能够打破同分布样本的假设，极大地增加机器学习的跨领域能力，提取更多的样本用于训练。而并非所有的迁移都会提升预测效果，无价值或不相关的信息的迁入只会造成预测效果大打折扣。

房晓南提出了半监督框架下的自标记技术和多学习器模型的结合方法，来解决欠标记且不平衡的垃圾网页数据集分类问题，是解决只有少量标记且类不平衡数据集分类问题的一个有效策略。杨印卫选取了支持向量机（SVM）、隐马尔可夫模型（HMM）以及径向基函数（RBF）神经

网络这3个单学习器作为异构集成学习模型的基分器，同时采用了majorityvoting和stacking两种集成结果整合策略来选择最优组合，证明异构集成学习模型的泛化能力相比于以往单分类模型得到了改善，同时模型复杂度降低。张伟利用卷积神经网络（Convolutional Neural Network，CNN）识别振动信号的时域图，引入了自适应批量归一化算法，取得了良好的变负载自适应性能。

郭勇给出了种基于简单投票制的样本迁移学习方法，有效提高了目标领域预测学习器的分类效果。对TrAdaBoost算法进行了权值更新策略方面的改进，解决了TrAdaBoost算法源领域与目标领域样本权值之间易出现的两极分化问题。然后以基于迁移成分分析（Transfer Component Analysis，TCA）的域匹配算法为基础，对其进行了归纳式扩展，并结合聚类算法对源领域数据集数据分布作进一步修正。

上述算法并未同时解决负迁移以及过拟合的问题，基于此，本文采用基于迁移学习的半监督集成算法，利用高相似度样本的迁移提高半监督学习的精度，同时利用半监督算法筛选迁移学习的高相似度样本，最大限度地防止负迁移的发生。

# 1 机器学习

# 1.1 半监督学习

在半监督的发展中，最早提出的是自训练以及直推学习，但训练出的学习器鲁棒性极差，然后是协同训练算法，这种算法的一个主要困难是它需要2个完全独立的视图，在大多数的学习问题中很难满足这一要求。此外，在协同训练中，值得信赖的样本的估计是通过交叉验证进行的，这是一个耗时的过程。为了克服这些困难，Zhou和 $\mathrm{Li}^{\sharp}$  提出了三重训练算法，三重训练的思想是训练3个训练集，依此3个训练集生成3个学习器，而且无需满足苛刻的独立条件，然后使用其他2个学习器同意的未标记数据对另一个学习器进行改进，由于未标记数据的分类误差估计比较困难，因此，在未标记数据与标记数据具有相同分布的前提下，仅对标记数据进行分类误差估计。3次训练过程的训练一直持续到误差停止减小为止，这意味着已经达到了最大的泛化效果。在一定的理论证明限制下，将一致的未标记样本逐步加入到标记数据中，用于细化相应的学习器，直到没有一个学习器的预测误差进一步减小，一旦训练过程完成，就可以用2个或多个成员学习器一

致同意的标签来预测未标记或看不见的数据

# 1.2 迁移学习

迁移学习的目的在于将已有的知识恰当地引入到新领域中，使机器能够获得“举一反三”的能力。本文将源领域  $D_{s}$  定义为“一”，将目标领域  $D_{\mathrm{T}}$  定义为“三”，也就是将  $D_{s}$  的学习经验迁移到  $D_{\mathrm{T}}$  中来。其中迁移的有效性很大程度上依赖于领域间的相似程度，当相差过大时，就会发生负迁移，降低学习性能。迁移学习可以划分为5类：基于实例的迁移，基于参数的迁移，基于特征的迁移，给予相关知识的迁移以及基于模型的迁移。

# 1.3 基于强化学习的极限学习机

极限学习机（FLM）是一种神经网络监督学习算法，采用基于有限集的局部逼近，收敛速度快，可以代替强化学习中  $Q$  值等于实现状态动作到  $Q$  值的映射。  $Q$  学习是强化学习的一种，在模型未知时通过估计  $Q$  值来实现目的。

$Q$  值函数更新公式为

$$
\begin{array}{r l}{Q_{t + 1}\big(s_{t},\pmb{a}_{t}\big)} & {= \big(1 - \eta \big)Q\big(s_{t},\pmb{a}_{t}\big) + }\\ {\eta \big[ \pmb{t}_{t} + \gamma \max_{a_{t + 1}}Q_{t}\big(s_{t},\pmb{a}_{t + 1}\big)\big]} \end{array} \tag{1}
$$

式中：  $\eta$  为学习速度；  $\gamma$  为折扣因子，影响决策的远视程度；  $s_t$  为  $t$  时刻的状态；  $\pmb{a}_{t}$  为  $t$  时刻的动作； $r_t$  为  $t$  时刻奖赏；  $\max_{a_{t + 1}}Q_t\big(s_{t + 1},a_{t + 1}\big)$  从  $Q$  值映射表查找得到。

随机设定输入权值矩阵  $\alpha$  以及偏置项  $\beta$  ；计算隐含层输出矩阵  $H$  ，以及输出量  $Q = Hw_{1},w_{1}$  为权值向量，  $\mathcal{Q} = \left[Q_{1},Q_{2},\dots ,Q_{L}\right]^{\mathrm{T}}$  为对应  $L$  个动作的估计  $Q$  值向量，再通过广义逆矩阵获取权值  $\mathfrak{w}_1$  。

# 1.4 集成学习

集成学习是一种机器学习方法，最初，集成学习的提出是由于Schapire证明了多个弱学习器可以形成一个强学习器，集成学习的目的是通过组合多个学习器的输出架构造多个不同的学习器，如图1所示。所谓“集成”是专家的混合体，用以防止过度拟合以及减少所有基础学习器的误差，而如何结合多种学习器的输出结果和提高基学习器的多样性来提高学习器的精度是重点。为了提高集成的精度和稳定性，人们开发了不同的技术。这些技术因所使用的培训数据、所使用算法的类型以及所遵循的组合方法而有所不同。所谓的异构集成学习器就是构造不同种类的基学习器，然后进行集成，多重训练的异构集成学习器便是取多个基学习器分别训练学习，最后对所有预测结果进行组合。

异构集成学习器的关键是根据所有基学习器

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/70714b30e4fbfa267cda036640c571305b82318d2525d31f82cd9a1de74ee9f6.jpg)  
图1 集成学习示意图

Fig.1 Schematic diagram of integrated learning

的预测来量化未标记样本的置信度，而精度也随学习器的数量增加而增加，而且对所有的训练集以及算法没有要求，避免了大量交叉验证。

# 2 基于迁移学习的支持向量机与极限学习机

# 2.1 基于迁移学习的支持向量机

一般来讲基于SVM的迁移学习直接利用2个领域的数据构建模型，但是面临着目标方程很难优化的问题，并且无法有效剔除源领域中的噪声影响，随着源领域数据的增多，算法复杂度将大大提高，由此本文提出筛选源领域相似度高的数据进行迁移的SVM算法，思想如下：

源领域与目标领域的高相似度势必会导致2个分类超平面过于接近，本文选取决定2个领域分类超平面的支持向量来进行筛选数据，从而最大程度降低负迁移的发生。源领域  $D_{\mathrm{S}}$  ，以及目标领域  $D_{\mathrm{T}}$  分别为

$$
\begin{array}{rl} & D_{\mathrm{S}}^{\mathrm{s}} = \left\{\left(x_{1}^{\mathrm{s}},y_{1}^{\mathrm{s}}\right),\left(x_{2}^{\mathrm{s}},y_{2}^{\mathrm{s}}\right),\dots ,\left(x_{m}^{\mathrm{s}},y_{m}^{\mathrm{s}}\right)\right\} \\ & D_{\mathrm{T}}^{\mathrm{t}} = \left\{\left(x_{1}^{\mathrm{t}},y_{1}^{\mathrm{t}}\right),\left(x_{2}^{\mathrm{t}},y_{2}^{\mathrm{t}}\right),\dots ,\left(x_{m}^{\mathrm{t}},y_{m}^{\mathrm{t}}\right)\right\} \end{array} \tag{2}
$$

如果本文借鉴欧氏距离的概念定义了一个函数  $\sigma (V_{\mathrm{S}}^{\mathrm{t}},D_{\mathrm{T}}^{\mathrm{t}})$  来衡量数据的相似性，其中  $V_{\mathrm{S}}^{\mathrm{t}}$  为源领域的支持向量，  $\beta$  用于控制  $V_{\mathrm{S}}$  的重要程度，  $\| V_{\mathrm{S}}^{\mathrm{t}} - x_{\mathrm{i}}\|_{2}^{2}$  表示  $V_{\mathrm{S}}$  与  $D_{\mathrm{T}}^{\mathrm{t}}$  的距离。

$$
\sigma (V_{\mathrm{S}}^{\mathrm{t}},D_{\mathrm{T}}^{\mathrm{t}}) = \frac{1}{k}\sum_{(x_{i},y_{i})\in D_{\mathrm{T}}^{\mathrm{t}}}\exp \big(-\beta \| V_{\mathrm{S}}^{\mathrm{t}} - x_{i}\|_{2}^{2}\big) \tag{4}
$$

式中：  $k$  为目标训练集个数。

具体步骤如下：

1）使用  $D_{\mathrm{S}}$  中的数据训练初始SVM，得到支持向量  $V_{\mathrm{S}}$  ，并计算相似度  $\sigma (V_{\mathrm{S}}^{\mathrm{t}},D_{\mathrm{T}}^{\mathrm{t}})$  。

2）将  $V_{\mathrm{S}}$  加入到源领域数据的已标记数据中，将相似度函数考虑进支持向量机的目标函数中，构造出如下新的优化问题：

$$
\left\{ \begin{array}{ll}\min_{\mathbf{w}}0.5\left\| \mathbf{w}\right\|_{2}^{2} + C\sum_{i = 1}^{k}\epsilon_{i} + \sum_{i = 1}^{M}\sigma (V_{\mathrm{s}}^{\mathrm{t}},D_{\mathrm{T}}^{\mathrm{t}})\bar{\epsilon}_{j} \\ \mathrm{s.t.} \\ y_{i}(\mathbf{w}^{\mathrm{T}}\mathbf{x}_{i} - b)\geqslant 1 - \epsilon_{i} & \epsilon_{i}\geqslant 0,\forall (\mathbf{x}_{i},\mathbf{y}_{i})\in D_{\mathrm{T}}^{\mathrm{t}} \\ y_{i}^{\mathrm{t}}(\mathbf{w}^{\mathrm{T}}\mathbf{v}_{\mathrm{S}}^{\mathrm{t}} - b)\geqslant 1 - \bar{\epsilon}_{j} & \bar{\epsilon}_{j}\geqslant 0,\forall (\mathbf{v}_{\mathrm{S}}^{\mathrm{t}},\mathbf{y}_{\mathrm{S}}^{\mathrm{t}})\in V_{\mathrm{S}} \end{array} \right. \tag{5}
$$

式中：  $\mathbf{w}$  为分类超平面权值向量；  $b$  为偏置。

3）将支持向量通过惩罚项  $C$  与目标领域形成新的训练集  $D$  用以训练新的学习器，引入拉格朗日系数将上述约束问题转化为对偶问题：

$$
\left\{ \begin{array}{ll}\max L(\alpha) = \sum_{i = 1}^{M + k}\alpha_{i} - 0.5\sum_{i = 1}^{M + k}\sum_{j = 1}^{M + k}\alpha_{j}\alpha_{j}y_{i}y_{j}\big(\mathbf{x}_{i}\bullet \mathbf{x}_{j}\big)\\ \mathrm{s.t.}\\ \epsilon_{i}\geqslant 0,\beta_{i}\geqslant 0,0\leqslant \alpha_{i}\leqslant C\tilde{\sigma}\big(\mathbf{x}_{i},D_{\mathrm{T}}^{\mathrm{t}}\big)\\ y_{i}\big(\mathbf{w}^{\mathrm{T}}\mathbf{x}_{i} - b\big)\geqslant 1 - \epsilon_{i},\beta_{i}\epsilon_{i} = 0\\ \alpha_{i}\big[\mathbf{y}_{i}\big(\mathbf{w}^{\mathrm{T}}\mathbf{x}_{i} - b\big) - 1 + \epsilon_{i}\big] = 0,\forall (\mathbf{x}_{i},\mathbf{y}_{i})_{i}\in D_{\mathrm{T}}^{\mathrm{t}} \end{array} \right.
$$

式中：  $M$  为已标记样本数量。

4）求解上述问题得到最优解  $\alpha^{*}$  ，以及最优分类平面权向量  $\mathbf{w}^{*},\mathbf{b}$  以及最终分类函数  $f(x)$  如下，根据  $f(x)$  ，得到最终学习器。

$$
\left\{ \begin{array}{ll}\boldsymbol{w}^{*} = \sum_{j = 1}^{M + k}\boldsymbol{\alpha}^{*}\boldsymbol{x}_{i}\boldsymbol{y}_{i} \\ b = \boldsymbol{y}_{i} - \boldsymbol{\epsilon}_{i} - \boldsymbol{w}^{*\mathrm{T}}\boldsymbol{x}_{i} \\ f(\boldsymbol {x}) = \mathrm{sign}\left(\sum_{j = 1}^{M + k}\boldsymbol{y}_{i}\boldsymbol{\alpha}_{i}^{*}(\boldsymbol{x}_{i}\bullet \boldsymbol {x}) + b\right) \end{array} \right. \tag{7}
$$

# 2.2 基于迁移学习的极限学习机

基于强化学习的迁移极限学习机通过贝叶斯理论计算目标任务与源任务的相似度，依此衡量迁移的数量，继续求出源任务各样本属于目标样本的概率并降序排列确定有效迁移样本。

迁移学习的核心在于选择合适的源任务添加进训练集中，本文通过计算源任务与目标领域的相似度来对源任务目标进行排序筛选。具体方式如下。

首先定义  $S_{i}$  为源任务，  $T$  为目标任务，  $\hat{s}_K$  为第  $K$  个源任务，  $\hat{\epsilon}_{ij}^{P}$  与  $\hat{\epsilon}_{ij}^{Q}$  分别表示状态相似度以及回报相似度，  $\omega_{ij}$  为相似度权值，  $\hat{T}$  为目标样本总数，  $\hat{\delta_{j}^{s_{K}}} = \left(s_{j},a_{j},s_{j}^{*},Q_{j}\right)$  为第  $K$  个源任务中第  $j$  个样本，  $\hat{\sigma_{i}^{T}} = \left(s_{i},a_{i},s_{i}^{*},Q_{i}\right)$  为目标任务中第  $i$  个样本，  $\hat{\delta^{s}}$  为高斯核宽度参数，  $P(S_{k})$  为  $S_{K}$  的先验概率，  $Z_{X}$  为似然度估计函数，与  $Z_{P}$  和  $Z_{Q}$  具有相同的功能，通常取1。

具体步骤如下：

1）执行动作  $a_{i}$  ，由式（8）和式（9）计算第  $K$  个源任务中第  $j$  个样本与目标任务中第  $i$  个样本的状态相似度以及回报相似度：

$$
\begin{array}{r}\xi_{ij}^{P} = \omega_{ij}\bullet \exp \Big[-\Big(\frac{\big|s_{i},s_{j}^{*}\big|\big|}{\delta^{s}}\Big)^{2}\Big] \\ \xi_{ij}^{Q} = \omega_{ij}\bullet \exp \Big[-\Big(\frac{\big|Q_{i},Q_{j}\big|\big|}{\delta^{s}}\Big)^{2}\Big] \end{array} \tag{9}
$$

2）计算第  $K$  个源任务中第  $j$  个样本属于目标任务的概率：

$$
P\big(\hat{\delta_{j}^{s_{K}}}\big|\hat{T}\big) = P_{T}\big(s_{j}^{s}\big|s_{j},a_{j}\big)\cdot P_{T}\big(Q_{j}\big|s_{j},a_{j}\big) =
$$

$$
\frac{1}{Z_{P}Z_{Q}}\left(\sum_{i = 1}^{n_{T}}\xi_{ij}^{P}\right)\left(\sum_{i = 1}^{n_{T}}\xi_{ij}^{Q}\right) \tag{10}
$$

式中：  $n_{T}$  为从  $T$  个训练集中已迁移样本数。

3）获取任务相似度，并且从每一个源任务中按照相似度排序选取（删除“个”）样本到目标样本集：

$$
X_{K} = P\big(S_{K}\big|\hat{T}\big) = \frac{1}{Z_{X}}\prod_{u = 1}^{n_{T}}\chi_{u}\cdot P\big(S_{K}\big) \tag{11}
$$

式中：  $\chi_{u}$  为状态回报相似度的匹配度。

4）使用新的训练集对ELM进行训练更新。

循环以上步骤直到达到最大迭代次数。

# 2.3 基于迁移学习的三重训练学习器

在Tri- Training的基础上引入迁移学习，需要强学习器对源领域样本进行筛选，比较弱学习器选择的样本进行迭代更新，通过判断弱学习器选择的样本与上一轮是否一致来调整样本权重以及对目标任务学习的影响，这里不再赘述具体步骤[3]。

# 3 基于迁移学习的半监督集成学习器

集成学习可以综合多个基学习器的优势，这是其优势，但会因此集成到对结果有负面影响的基学习器，因此本文需要为基学习器设置权值，并根据错误率实时更新权值最大限度地降低基学习器的负面作用。

根据图2的学习器构造方式，集成最终的学习器。

参考图2，具体的步骤如下：

步骤1通过boosting方法将已标记样本  $L(x_{i},y_{i})$  构造为具有差异性训练集的L1到L6，

同样的方法将未标记样本  $U(x_{j})$  构造得到训练集U1到U3，定义目标领域样本集D1到D3。

步骤2根据训练集L1到L3分别训练得到基学习器SVM、ELM、Tri- training。再按照上述改进的迁移学习算法通过训练集L2和D1、L4和D2、L6和D3分别获得迁移学习的支持向量机（TSVM）、基于迁移学习的极限学习机（TELM）和基于迁移学习的三重训练学习器（TTri- training）。

步骤3将未标记样本U1通过SVM以及TSVM进行识别，若两学习器对样本  $x_{1j}$  判别结果相同都为  $y_{1j}$  则将样本分类结果  $(x_{1i},y_{1i})$  添加到L7中，同理将U2通过ELM以及TELM进行识别，获得已标记样本L8，同理获得L9。

步骤4将L7、L1以及D1中迁移学习过程中认可的目标样本共同训练得到半监支持向量（Semi- supervised Support Vector Machines, SS- VM）、半监督极限学习器（Semi- supervised Extreme Learning Machine, SELM）以及半监督三重训练学习器（Semi- supervised Triple training learning machine, STri- training）。

步骤5将U4  $(x_{ij})$  通过3个半监督基学习器SSVM、SELM以及STri- training进行识别，按照式（12）集成其识别结果，将集成结果加入到L10  $(x_{10k},y_{10k})$  中，将L10用于测试基学习器的错误率  $\epsilon_{i}$  ，如式（13），并以错误率获得基学习器权重  $\omega_{i}$  。

$$
\begin{array}{rcl}H(y) & = & \arg \max \sum_{j = 0}^{3}\omega_{j}\big(f_{j}(y) = H\big)\\ \epsilon_{i} & = & \sum_{k = 1}^{n}\omega_{i}\big|y_{10k} - h_{ik}\big(x_{10k}\big)\big|\\ \omega_{i + 1} & = & \frac{\mathrm{e}^{1 - \epsilon_{i}}}{\mathrm{e}^{\epsilon_{i}}} \end{array} \tag{14}
$$

式中：  $H(y)$  为预测结果函数；  $\omega_{j}$  为第  $j$  个半监督集成学习器的权重，等于其2个同簇初始基学习器权重  $\omega_{i}$  相加；  $h_{ik}\big(x_{ik}\big)$  为第  $i$  个初始基学习器对L10中第  $k$  个样本的识别结果。

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/daf91b05e36f12e6cca4452672c98c3593d1f70ab253b880ab10c9f39bb8ff50.jpg)  
图2学习器构造方式图 Fig.2 Learning device structure diagram

步骤6 不断调整学习器权重并实时记录学习器误差，直到权重以及误差都稳定得到稳定的半监督基学习器，并按照稳定后的权值集成最终学习器。

在上述算法中，为便于理解，通过示意图3阐述在上述步骤中使用的半监督学习算法流程，图4为TSVM的构造方式，图5为TELM的构造方式。其中集成算法的步骤可参考文献[1]。

相比于其他算法，基于迁移学习的集成学习器的优点有：

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/fac6f2c98fe2b5d5b4baddb6bdfb47cb46c3a42cb02196b309ee3279338025ca.jpg)  
图3 半监督算法构造示意图

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/1d0aff7a86044b818ead5532bcd9ddf2acf03b6fb86b57394c22854ce90dcbf8.jpg)  
Fig.3 Schematic diagram of semi-supervision algorithm construction  图4 TSVM算法示意图 Fig.4 Schematic diagram of TSVM algorithm

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/e0e4f9008ef25d78a65dc6437e3fe272bbd3d44c9f4fc74add36e753f17ba16a.jpg)  
图5 TELM算法示意图 Fig.5 Schematic diagram of TELM algorithm

1）提出改进迁移学习器加入到基学习器中。2）利用改进的迁移学习器提高半监督学习的精度。3）利用半监督学习调整迁移学习的效率。4）集成方式上可以在运算中调整各学习器权值，进行增量式学习，进一步提升学习器性能。

# 4 仿真实例

为验证算法的有效性，本文在2个数据集中分别进行实验。在字母识别数据集中，主要验证算法在分类识别上的效果，量化半监督学习方法与迁移学习方法间的相互影响，调整算法中的参数以及分析各基分类器的精度变化。在发动机轴承数据集中主要验证本文方法在轴承故障识别的效果，寻找最优的迁移数据以及半监督数据。

# 4.1 公共数据集实验

为了测试最终学习器的性能，本文在机器学习存储库（UCI）的字母识别数据集上进行了模拟。选取26个字母中A～E个字母为源任务，选取  $\mathrm{F}\sim \mathrm{N}$  为目标领域样本，具体的样本特征如表1所示。

实验采用RBF为基分类算法，隐藏节点取20，每次实验做5次取其平均值，设定初始基学习器的权重都为1/6，已标记样本数量、未标记样本与迁移样本比例为  $1:4:4$  。安排单纯的半监督神经网络以及基于迁移学习的学习器作为对比。

图6为误差稳定后基学习器以及集成学习器的预测精度变化。从图中可见，分类效果经过迁移学习以及半监督学习后有显著的提升，无论是稳定性还是精度都大幅优于单一的学习器，而且经过少量的样本的训练后精度就达到了良好的效果，而单独的迁移学习或半监督学习都无法做到。这是由于基于迁移学习的半监督集成学习器（SSIT）可以更充分利用样本的信息，无需大量迭代就会达到良好的效果。

# 表1 数据集属性

Table 1 Data set properties  

<table><tr><td>数据集</td><td>字母</td><td>样本数</td><td>字母数</td></tr><tr><td>L</td><td>A～E</td><td>100</td><td>5</td></tr><tr><td>U</td><td>A～E</td><td>400</td><td>5</td></tr><tr><td>L1-L6</td><td>A～E</td><td>30</td><td>5</td></tr><tr><td>U1-U3</td><td>A～E</td><td>120</td><td>5</td></tr><tr><td>D1</td><td>F,G,H</td><td>400</td><td>3</td></tr><tr><td>D2</td><td>I,G,K</td><td>400</td><td>3</td></tr><tr><td>D3</td><td>L,M,N</td><td>400</td><td>3</td></tr><tr><td>合计</td><td>A～N</td><td>1 850</td><td>29</td></tr></table>

图7为基学习器权重变化，经由错误率的调整后可以看出，迁移学习器的权重都由于可以提取目标样本的信息而逐渐提高，三重训练器权重不断拔高。若本文保持初始权值恒定，预测精度会下降大约  $8\%$  。据此本文也调整初始学习器权重，使其在训练过程中减轻权重变化的迭代，可以更快地达到稳定的预测精度。

本文通过调整实验找到精度提升的原因。图8为调整未标记样本  $U$  的数量，对比迁移学习器的精度，可以发现无标记样本对于迁移学习的效果有较大的影响，在没有无标记样本时，也就是不进行半监督学习时，预测精度下降了大约  $10\%$  。随着无标记样本的增加，迁移学习器精度以及迁移数量都会稳步提升，这是因为无标记样本的训练信息可以筛选目标领域中的高相似度样本，保证迁移学习的效果。但精度的增加有一个上限，这是由于少量的有标记样本仍然是训练的核心，仍然会有过拟合的作用。数量的增长也有上限，因为目标领域内高相似度样本数量也有限。

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/f3ee8abe20c0a27c69b41462c3ca88022bbb0cc253ce84c8f1f9ac9092b60f08.jpg)  
图6 预测精度变化

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/c6c5f2c38a5db24524011846e4c661a873ad1c6c62c9574bf417d9c7ba09ecb6.jpg)  
图7 基学习器权重变化

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/2d947cbca57d06fe23c63a2ce1a41409f35ebdc9d711229364d20c0c6805bda7.jpg)  
Fig.7 Base learning device weight change  图8迁移学习器精度变化 Fig.8 Transfer base learning device accuracy change

同理本文保持  $U$  的数量恒定，调整  $D$  中的样本数量，如图9和图10所示，发现若是没有迁移目标领域的样本，半监督学习效果的稳定性很差，最大最小精度之间有较大差异，这是由于半监督学习一旦做出错误的判断便会“一错再错”，自身很难纠正，需要目标领域样本来调整，随着迁移样本的增加，预测精度稳步增加且趋于稳定，最大最小精度差异也下降到  $2\%$  左右。

继续调整已标记样本的比例，图11为SSIT学习器精度变化。已标记样本的增加固然可以提高辨识度，但过多的已标记样本可以保持最高精度，但平均精度反而有所下降，这是由于学习器过于追求少量的已标记样本，过拟合效果增大，鲁棒性也下降，反而不易对大量复杂的样本进行预测。在已标记样本比较少时，SSIT仍然表现表现出稳定的预测性能，而单独的半监督或迁移学习算法却受到极大影响，这是因为SSIT并不依赖那些少量的已标记样本，可以从目标领域以及未标记样本中提取到充足的信息来保证预测精度。

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/b0fac852c7a332c5c9ef39db89d75afe52effd786dda0017858630882b199e7d.jpg)  
图9迁移数量变化 Fig.9 Transfer data change

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/e62628821496c453625296e4e06862b2a0ec976d19e58794c872e33fc9406fc4.jpg)  
图10半监督学习精度变化 Fig.10 Semi-supervised learning accuracy change

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/7659b3b892197e60018dcbb68c3dc668905902ec0df5fa840355e1c99f139939.jpg)  
图11 精度随已标记样本比例变化 Fig.11 Change of accuracy with labeled sample scale

# 4.2 民航发动机轴承故障诊断实验

在公共数据集上的实验验证了所提算法对迁移以及半监督效果的改进作用，但对于具体的发动机内数据效果犹未可知，接下来本文搭建接近航空发动机内部的轴承振动试验设备对轴承故障进行诊断，实验具体设置如下。

本文搭建航空发动机机转子实验平台进行模拟实验，其模拟了真实发动机的内部结构，达到真实发动机的振动效果，图12为发动机实验平台内部结构，图13为迁移实验平台示意图。选择型号6205的轴承，滚动体9个。在模拟发动机50000磅（22.68t）推力下通过加速度传感器收集3种轴承故障（内圈、外圈、滚动体）以及正常情况下的轴承振动数据各1000组。

模拟发动机最大推力63800磅下轴承振动数据500组作为迁移领域D1，模拟50000磅推力下同位置6206型号轴承振动数据500组为D2，取来自凯斯西储大学轴承数据库型号6205轴承在负载  $746~\mathrm{W}$  、转速  $2000~\mathrm{r / min}$  下轴承振动数据500组为D3。

首先利用包络谱的方法提取振动信号的具体特征，其中载波频率为  $5000\mathrm{Hz}$  ，将通过得到的

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/48f45040bdbcd97af928729b148b324db9f0860d8586c08e5206c862e35031ef.jpg)  
图12 发动机转子实验平台内部结构

Fig.12 Internal structure of engine rotor experimental platform

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/4870ca86342c4fb9bab0c27ed59a322095a757ca3e3af0bd12140c65bb2d53b0.jpg)  
图13 迁移轴承实验平台示意图

Fig.13 Schematic diagram of transfer bearing test rig

分量组成特征矩阵并采用奇异值分解方法求得对应的特征向量，于是每个样本信号都有对应的特征向量作为训练的输入。图14和图15分别为内圈故障振动信号时域波形以及包络图。

通过上述方法将振动信息提取为特征向量，得到带有故障结果的轴承振动特征向量数据集 $L$  ，以及无故障结果的数据集  $U$  以及迁移数据  $D$  0为检验算法的有效性，本文采用同样数量已标记样本的训练样本以及相同的测试样本，分别使用基学习器、CNN以及基于距离的谱乘类（SL）算法进行对比测试，其中采用同样特征提取方法将特征向量的输入到训练好的TELM中进行诊断，再利用卷积神将网络提取振动信号的包络谱进行学习，保证特征的提取程度不变。

将训练成熟后的学习器输入测试样本的特征向量，根据输出向量的值进行故障预测，其中信号类型中1、2、3、4等分别代表无故障，内圈故障，外圈故障以及滚珠故障。图16为在不同方法对样本的预测结果与实际结果的对比

选取不同的测试样本分别测试5次，表2为在不同数据集合方法的精度比较，训练过后的SSIT的平均精度达到了  $90.6\%$  ，远高于CNN等预测精度，虽然CNN方法对于大量的图片识别率基本上达到了  $99\%$  以上的识别率，但对于航空发动机的轴承振动时域图却因为已标记样本信息过少且特征复杂的原因而大大降低了识别率，SSIT

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/782ecf478ff3a87ed7ae1367755f65e2eaec35cd8cbc10468d7615a55145248c.jpg)  
图14 时域波形图

Fig.14 Time domain waveform

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/52235bb1239b4e7cfefd33af0e4028a37a0dd8dac8d5527977343d9e0a5c00da.jpg)  
图15 包络谱分析

Fig.15 Envelope spectrum analysis

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/8c7e79cf65f4d9991cb3fe8aef69dd56f59c634f20ebe347e0ba2ae54720b27b.jpg)  
图16 不同方法对故障的预测结果 Fig.16 Fault prediction result by different methods

却可以从大量的源领域以及未标记样本中获得辅助信息帮助判断。TELM以及SELM的低预测精度也表明半监督以及迁移学习在面对少量已标记样本时有着相辅相成缺一不可的关系。

本文选取3个数据集，比较3个迁移领域最终的迁移样本量，3个领域的区别在于工作负载以及环境的差异，差异越大越难以迁移，从表3中可以看出，实验状态下D3数据集的轴承振动数据难以用于预测民航发动机故障预测，而这正是现如今大量轴承预测方法难以在发动机环境内

# 表2 不同数据集各方法预测精度对比

Table 2 Comparison of prediction accuracy of different methods on different data sets %  

<table><tr><td>测试集</td><td>SSIT</td><td>CNN</td><td>TELM</td><td>SL</td></tr><tr><td>测试集1</td><td>90.5</td><td>81.3</td><td>68</td><td>69.7</td></tr><tr><td>测试集2</td><td>90.6</td><td>82.1</td><td>68</td><td>69.6</td></tr><tr><td>测试集3</td><td>90.6</td><td>82.2</td><td>72</td><td>68.5</td></tr><tr><td>测试集4</td><td>90.5</td><td>79.2</td><td>71</td><td>70.4</td></tr><tr><td>测试集5</td><td>90.6</td><td>79.5</td><td>73</td><td>69.5</td></tr><tr><td>平均值</td><td>90.6</td><td>80.9</td><td>70</td><td>69.5</td></tr></table>

# 表3 最终迁移样本百分比

Table 3 Percentage of final migrated sample %  

<table><tr><td>测试集</td><td>D1</td><td>D2</td><td>D3</td></tr><tr><td>测试集1</td><td>63.2</td><td>40.1</td><td>22.3</td></tr><tr><td>测试集2</td><td>69.4</td><td>43.9</td><td>20.3</td></tr><tr><td>测试集3</td><td>66.7</td><td>39.6</td><td>25.1</td></tr><tr><td>平均值</td><td>66.4</td><td>41.2</td><td>22.6</td></tr></table>

奏效的原因，D1于D2则展现了与源领域样本的高相似度，对于SSIT的故障预测给予了巨大帮助，由此本文可以增加同发动机不同推力下的目标领域样本以及不同轴承型号相同推力下的样本，以期迁移更多的样本帮助预测

图17为在D1领域下，迁移样本量的提升，SSIT精度的精度变化。从图中可以看出，在迁移样本达到350个以后，迁移学习提升了整个学习器  $18\%$  的精度。

半监督算法中的未标记样本选用实验前已记录的无结果振动数据，但是本文在实验中发现如果加入部分测试时间点前后的振动信息作为未标记样本参与训练会提升算法的精度以及稳定

性。而无法确定的是多少量的已记录样本（HS），测试前样本（BS），测试后样本（AS）能够最大提升精确性，本文不断调整无标记样本中三者的比例来测试其精确度的变化。如图18所示。可以看出加入的测试节点样本有效地提升了精度，并且在（4.84，6.58）达到最大值，这是因为未标记样本中加入测试节点前后的振动数据，可以在训练时完善预测节点的特征，特别是在航空发动机内的轴承振动信息，复杂的噪声下不易提取到精确的振动信号特征；而未标记样本的半监督训练相当于又一次进行了特征提取，提高了预测的精度。

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/68d277b4e9c1dc6b65eb103dcca5f8fe057715126c1db4e95433acb164b28bd2.jpg)  
图17 D1下迁移样本量对精度的影响

Fig.17 Effect of transfer sample size on accuracy under D1

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-13/90c8b9f5-ac58-48b6-80ee-8e1aca32458a/3cecf20cb361bb6366eba0aeb247565d128e60d4ffce00db4b229d9ae2491c51.jpg)  
图18 测试前后样本量对精度的影响 Fig.18 Effect of HS and BS on prediction accuracy

# 5 结论

本文提出了一种半监督迁移学习集成学习器，构造改进迁移学习器组成6种初始基学习器来集成同簇的半监督基学习器，利用迁移学习降低半监督学习中的不稳定性，再利用无标记样本减少负迁移作用，并不断调整基学习器权重，提升SSIT的精度与稳定性，得出如下结论：

1）迁移学习可以提高半监督学习的稳定性以及降低过拟合效果。2）半监督学习可以帮助迁移算法挑选更多更好的高相似度样本。3）同簇集成的方法也可以通过调整权重的方法减少负学习的效果。4）温和环境下的轴承振动数据难以直接迁移至发动机内部的轴承预测中，而不同推力以及不同发动机下的振动数据则展现了良好的迁移效果。

5）测试节点前后的数据加入到无标记样本的半监督训练中可提升最终的预测精度。

将研究更加多部件故障的迁移方法，并将其运用到发动机其他部件的故障预测方面，利用大量的目标领域样本来提高预测精度。