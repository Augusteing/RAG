# -*- coding: utf-8 -*-
"""
æµ‹è¯• BGE-large-zh-v1.5 æ¨¡å‹åœ¨ RTX 3060 ä¸Šçš„è¿è¡Œæƒ…å†µ
"""
import torch
from sentence_transformers import SentenceTransformer
import time

print("=" * 60)
print("ğŸ” æ£€æŸ¥ GPU ç¯å¢ƒ")
print("=" * 60)

# æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨
print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"âœ… CUDA æ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"âœ… CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"âœ… GPU è®¾å¤‡: {torch.cuda.get_device_name(0)}")
    print(f"âœ… æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    print(f"âœ… å½“å‰æ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")

print("\n" + "=" * 60)
print("ğŸ“¥ åŠ è½½ BGE-large-zh-v1.5 æ¨¡å‹")
print("=" * 60)
print("â³ é¦–æ¬¡è¿è¡Œä¼šä» HuggingFace ä¸‹è½½æ¨¡å‹ï¼ˆ~1GBï¼‰ï¼Œè¯·ç¨å€™...")

start_time = time.time()

# åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨ä½¿ç”¨ GPUï¼‰
model = SentenceTransformer('BAAI/bge-large-zh-v1.5')

# å¦‚æœæœ‰ GPUï¼Œç¡®ä¿æ¨¡å‹åœ¨ GPU ä¸Š
if torch.cuda.is_available():
    model = model.to('cuda')
    print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° GPU")
else:
    print(f"âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU è¿è¡Œ")

load_time = time.time() - start_time
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f} ç§’")

if torch.cuda.is_available():
    print(f"âœ… åŠ è½½åæ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")

print("\n" + "=" * 60)
print("ğŸ§ª æµ‹è¯•ä¸­æ–‡ç¼–ç èƒ½åŠ›ï¼ˆPHM é¢†åŸŸç¤ºä¾‹ï¼‰")
print("=" * 60)

# æµ‹è¯•æ–‡æœ¬ï¼ˆæ¥è‡ªä½ çš„ä¾å­˜è·¯å¾„æ¨¡å¼ï¼‰
test_texts = [
    "æ–¹æ³• â†’ è§£å†³ â†’ é—®é¢˜",
    "æŠ€æœ¯ â†’ åº”ç”¨äº â†’ åº”ç”¨é¢†åŸŸ",
    "æ¨¡å‹ â†’ é¢„æµ‹ â†’ æ•…éšœ",
    "ç®—æ³• â†’ æé«˜ â†’ å‡†ç¡®ç‡",
    "ä¼ æ„Ÿå™¨ â†’ ç›‘æµ‹ â†’ è®¾å¤‡çŠ¶æ€",
    "æ·±åº¦å­¦ä¹ æ–¹æ³•ç”¨äºæ•…éšœé¢„æµ‹ä¸å¥åº·ç®¡ç†",
    "åŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„è½´æ‰¿æ•…éšœè¯Šæ–­ç ”ç©¶",
    "æ”¯æŒå‘é‡æœºåœ¨è®¾å¤‡å‰©ä½™å¯¿å‘½é¢„æµ‹ä¸­çš„åº”ç”¨"
]

print("â³ ç¼–ç æµ‹è¯•æ–‡æœ¬...")
start_time = time.time()
embeddings = model.encode(test_texts, show_progress_bar=True)
encode_time = time.time() - start_time

print(f"\nâœ… ç¼–ç å®Œæˆï¼")
print(f"   - æ–‡æœ¬æ•°é‡: {len(test_texts)}")
print(f"   - å‘é‡ç»´åº¦: {embeddings.shape[1]}")
print(f"   - æ€»è€—æ—¶: {encode_time:.3f} ç§’")
print(f"   - å¹³å‡é€Ÿåº¦: {len(test_texts)/encode_time:.2f} æ¡/ç§’")

if torch.cuda.is_available():
    print(f"   - å³°å€¼æ˜¾å­˜: {torch.cuda.max_memory_allocated(0) / 1024**3:.2f} GB")

print("\n" + "=" * 60)
print("ğŸ”¬ è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦")
print("=" * 60)

# è®¡ç®—å‰ä¸¤ä¸ªæ¨¡å¼çš„ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

sim_matrix = cosine_similarity(embeddings[:5])
print("\nè¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆå‰5æ¡æ¨¡å¼ï¼‰ï¼š")
print("æ–‡æœ¬1: æ–¹æ³• â†’ è§£å†³ â†’ é—®é¢˜")
print("æ–‡æœ¬2: æŠ€æœ¯ â†’ åº”ç”¨äº â†’ åº”ç”¨é¢†åŸŸ")
print("æ–‡æœ¬3: æ¨¡å‹ â†’ é¢„æµ‹ â†’ æ•…éšœ")
print("æ–‡æœ¬4: ç®—æ³• â†’ æé«˜ â†’ å‡†ç¡®ç‡")
print("æ–‡æœ¬5: ä¼ æ„Ÿå™¨ â†’ ç›‘æµ‹ â†’ è®¾å¤‡çŠ¶æ€")
print("\nç›¸ä¼¼åº¦çŸ©é˜µ:")
print(np.round(sim_matrix, 3))

print("\nğŸ’¡ è§£è¯»:")
print(f"   - æ–‡æœ¬1 vs æ–‡æœ¬2 ç›¸ä¼¼åº¦: {sim_matrix[0,1]:.3f}")
print(f"   - æ–‡æœ¬3 vs æ–‡æœ¬5 ç›¸ä¼¼åº¦: {sim_matrix[2,4]:.3f}")
print(f"   - æ¨¡å‹èƒ½å¤Ÿæ•æ‰è¯­ä¹‰ç›¸ä¼¼æ€§ï¼")

print("\n" + "=" * 60)
print("âœ… æµ‹è¯•å®Œæˆï¼BGE-large-zh-v1.5 åœ¨ä½ çš„ RTX 3060 ä¸Šè¿è¡Œæ­£å¸¸")
print("=" * 60)
print("\nğŸ“ æ€§èƒ½æ€»ç»“:")
print(f"   - GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")
print(f"   - æ˜¾å­˜å ç”¨: {torch.cuda.max_memory_allocated(0) / 1024**3:.2f} GB (å³°å€¼)")
print(f"   - ç¼–ç é€Ÿåº¦: {len(test_texts)/encode_time:.2f} æ¡/ç§’")
print(f"   - å‘é‡ç»´åº¦: 1024")
print(f"\nğŸ’¡ é¢„ä¼°:")
print(f"   - 2,287 æ¡æ¨¡å¼å‘é‡åŒ–: çº¦ {2287/(len(test_texts)/encode_time):.1f} ç§’")
print(f"   - 200 ç¯‡è®ºæ–‡æ£€ç´¢: æ¯ç¯‡çº¦ {encode_time/len(test_texts):.2f} ç§’")
