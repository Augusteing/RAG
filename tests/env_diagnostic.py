# -*- coding: utf-8 -*-
"""ç¯å¢ƒè¯Šæ–­è„šæœ¬"""
import sys
import os
from pathlib import Path

print("=" * 80)
print("ğŸ” PHM çŸ¥è¯†å›¾è°±é¡¹ç›® - ç¯å¢ƒè¯Šæ–­æŠ¥å‘Š")
print("=" * 80)

# 1. Python ç¯å¢ƒ
print("\nã€1ã€‘Python ç¯å¢ƒ")
print(f"   Python ç‰ˆæœ¬: {sys.version.split()[0]}")
print(f"   Python è·¯å¾„: {sys.executable}")
print(f"   Conda ç¯å¢ƒ: {os.getenv('CONDA_DEFAULT_ENV', 'N/A')}")

# 2. ç¡¬ä»¶ä¿¡æ¯
print("\nã€2ã€‘ç¡¬ä»¶é…ç½®")
try:
    import torch
    print(f"   âœ… PyTorch: {torch.__version__}")
    if torch.cuda.is_available():
        print(f"   âœ… CUDA å¯ç”¨: {torch.version.cuda}")
        print(f"   âœ… GPU: {torch.cuda.get_device_name(0)}")
        print(f"   âœ… æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    else:
        print(f"   âš ï¸ CUDA ä¸å¯ç”¨ï¼ˆä½¿ç”¨ CPU æ¨¡å¼ï¼‰")
except Exception as e:
    print(f"   âŒ PyTorch æœªå®‰è£…æˆ–æœ‰é—®é¢˜: {e}")

# 3. RAG æ ¸å¿ƒåº“
print("\nã€3ã€‘RAG æ ¸å¿ƒåº“")
try:
    import langchain
    print(f"   âœ… LangChain: {langchain.__version__}")
except Exception as e:
    print(f"   âŒ LangChain: {e}")

try:
    import chromadb
    print(f"   âœ… ChromaDB: {chromadb.__version__}")
except Exception as e:
    print(f"   âŒ ChromaDB: {e}")

try:
    import sentence_transformers
    print(f"   âœ… Sentence-Transformers: {sentence_transformers.__version__}")
except Exception as e:
    print(f"   âŒ Sentence-Transformers: {e}")

try:
    import pandas
    print(f"   âœ… Pandas: {pandas.__version__}")
except Exception as e:
    print(f"   âŒ Pandas: {e}")

# 4. LLM API åº“
print("\nã€4ã€‘LLM API åº“")
try:
    import openai
    print(f"   âœ… OpenAI SDK: {openai.__version__}")
except Exception as e:
    print(f"   âŒ OpenAI SDK: {e}")

# 5. ç¯å¢ƒå˜é‡æ£€æŸ¥
print("\nã€5ã€‘API å¯†é’¥é…ç½®")
api_keys = {
    "DEEPSEEK_API_KEY": os.getenv("DEEPSEEK_API_KEY"),
    "KIMI_API_KEY": os.getenv("KIMI_API_KEY"),
    "MOONSHOT_API_KEY": os.getenv("MOONSHOT_API_KEY"),
    "HIAPI_API_KEY": os.getenv("HIAPI_API_KEY"),
    "GEMINI_API_KEY": os.getenv("GEMINI_API_KEY"),
    "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY"),
}

for key_name, key_value in api_keys.items():
    if key_value:
        masked = f"{key_value[:6]}...{key_value[-4:]}" if len(key_value) > 10 else "***"
        print(f"   âœ… {key_name}: {masked}")
    else:
        print(f"   âš ï¸ {key_name}: æœªé…ç½®")

# 6. é¡¹ç›®ç»“æ„æ£€æŸ¥
print("\nã€6ã€‘é¡¹ç›®ç»“æ„")
project_root = Path(__file__).parent.parent
important_paths = {
    "è®ºæ–‡æ•°æ®": project_root / "data" / "raw" / "papers",
    "ä¾å­˜æ¨¡å¼CSV": project_root / "data" / "processed" / "dependency_patterns" / "semantic_syntactic_patterns_report_2025-10-14_172830.csv",
    "Schemaæ–‡ä»¶": project_root / "schema" / "phm_semantic_patterns.json",
    "å‘é‡æ•°æ®åº“": project_root / "data" / "vectorstores" / "pattern_chroma_db",
    "æŠ½å–è„šæœ¬": project_root / "src" / "extraction" / "extractors",
    "RAGè„šæœ¬": project_root / "src" / "rag",
}

for name, path in important_paths.items():
    if path.exists():
        if path.is_dir():
            try:
                count = len(list(path.iterdir()))
                print(f"   âœ… {name}: {path} ({count} é¡¹)")
            except:
                print(f"   âœ… {name}: {path}")
        else:
            size = path.stat().st_size / 1024
            print(f"   âœ… {name}: {path} ({size:.1f} KB)")
    else:
        print(f"   âš ï¸ {name}: {path} (ä¸å­˜åœ¨)")

# 7. æ€§èƒ½æµ‹è¯•
print("\nã€7ã€‘å¿«é€Ÿæ€§èƒ½æµ‹è¯•")
try:
    import torch
    import time
    if torch.cuda.is_available():
        # GPU æµ‹è¯•
        print(f"   æµ‹è¯• GPU è®¡ç®—...")
        x = torch.randn(1000, 1000, device='cuda')
        start = time.time()
        y = torch.matmul(x, x)
        torch.cuda.synchronize()
        elapsed = time.time() - start
        print(f"   âœ… GPU çŸ©é˜µè¿ç®— (1000x1000): {elapsed*1000:.2f} ms")
        print(f"   âœ… GPU æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")
    else:
        print(f"   âš ï¸ GPU ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
except Exception as e:
    print(f"   âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")

# 8. å»ºè®®
print("\n" + "=" * 80)
print("ğŸ’¡ ç¯å¢ƒè¯„ä¼°ä¸å»ºè®®")
print("=" * 80)

issues = []
suggestions = []

# æ£€æŸ¥é—®é¢˜
try:
    import torch
    if not torch.cuda.is_available():
        issues.append("GPU ä¸å¯ç”¨")
        suggestions.append("é‡æ–°å®‰è£… CUDA ç‰ˆæœ¬çš„ PyTorch")
except:
    issues.append("PyTorch æœªå®‰è£…")
    suggestions.append("å®‰è£… PyTorch: pip install torch --index-url https://download.pytorch.org/whl/cu118")

if not os.getenv("DEEPSEEK_API_KEY") and not os.getenv("OPENAI_API_KEY"):
    issues.append("ç¼ºå°‘ä¸»è¦ API å¯†é’¥")
    suggestions.append("é…ç½®è‡³å°‘ä¸€ä¸ª LLM API å¯†é’¥ï¼ˆDEEPSEEK_API_KEY æˆ– OPENAI_API_KEYï¼‰")

vector_db = project_root / "data" / "vectorstores" / "pattern_chroma_db"
if not vector_db.exists():
    issues.append("å‘é‡æ•°æ®åº“æœªæ„å»º")
    suggestions.append("è¿è¡Œ: python src/rag/build_pattern_vectorstore.py")

if issues:
    print("\nâš ï¸ å‘ç°çš„é—®é¢˜:")
    for i, issue in enumerate(issues, 1):
        print(f"   {i}. {issue}")
    
    print("\nğŸ’¡ å»ºè®®çš„ä¿®å¤æªæ–½:")
    for i, suggestion in enumerate(suggestions, 1):
        print(f"   {i}. {suggestion}")
else:
    print("\nâœ… ç¯å¢ƒé…ç½®å®Œç¾ï¼æ‰€æœ‰ç»„ä»¶å°±ç»ªã€‚")
    print("\nğŸš€ å¯ä»¥å¼€å§‹ RAG æµ‹è¯•:")
    print("   1. æ„å»ºå‘é‡æ•°æ®åº“: python src/rag/build_pattern_vectorstore.py")
    print("   2. æµ‹è¯• RAG æ£€ç´¢: python src/rag/test_retrieval.py")
    print("   3. é›†æˆåˆ°æŠ½å–æµç¨‹: ä¿®æ”¹ exact_*.py è„šæœ¬")

print("\n" + "=" * 80)
