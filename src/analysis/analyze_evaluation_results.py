# -*- coding: utf-8 -*-
"""
è¯„ä¼°ç»“æœç»Ÿè®¡åˆ†æè„šæœ¬
åˆ†æ Gemini å¯¹ä¸‰ä¸ªæ¨¡å‹æŠ½å–ç»“æœçš„è¯„ä¼°æ•°æ®
ç”Ÿæˆè¯¦ç»†çš„å‡†ç¡®ç‡ã€é”™è¯¯åˆ†æå’Œå¯¹æ¯”æŠ¥å‘Š
"""
import os
import json
import argparse
from pathlib import Path
from collections import defaultdict, Counter
import pandas as pd
from datetime import datetime

# ------------------------------
# è·¯å¾„é…ç½®
# ------------------------------
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent

# å®éªŒé€‰æ‹©ï¼šå‘½ä»¤è¡Œä¼˜å…ˆï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡ï¼Œé»˜è®¤ exp02
_parser = argparse.ArgumentParser(add_help=False)
_parser.add_argument("--exp", dest="exp_id", choices=["exp02", "exp03"], default=None)
_parser.add_argument("--exp02", dest="exp02", action="store_true")
_parser.add_argument("--exp03", dest="exp03", action="store_true")
_parser.add_argument("--03", dest="exp03_short", action="store_true")
_args, _unknown = _parser.parse_known_args()
_env_exp = os.getenv("EXP_ID")
if _args.exp_id in {"exp02", "exp03"}:
    EXP_ID = _args.exp_id
elif _args.exp03 or _args.exp03_short:
    EXP_ID = "exp03"
elif _args.exp02:
    EXP_ID = "exp02"
elif _env_exp in {"exp02", "exp03"}:
    EXP_ID = _env_exp
else:
    EXP_ID = "exp02"

# è¯„ä¼°ç»“æœç›®å½•ï¼ˆæŒ‰å®éªŒåˆ†æµï¼‰
EVAL_DIR = Path(f"E:/langchain/outputs/evaluations/{EXP_ID}")
DEEPSEEK_EVAL_DIR = EVAL_DIR / "deepseek"
GEMINI_EVAL_DIR = EVAL_DIR / "gemini"
KIMI_EVAL_DIR = EVAL_DIR / "kimi"

# è¾“å‡ºç›®å½•ï¼ˆæŒ‰å®éªŒåˆ†æµï¼‰
ANALYSIS_OUTPUT_DIR = Path(f"E:/langchain/outputs/analysis/evaluation_results/{EXP_ID}")
os.makedirs(ANALYSIS_OUTPUT_DIR, exist_ok=True)

# ------------------------------
# åˆ†æå‡½æ•°
# ------------------------------
def analyze_single_file(eval_file: Path) -> dict:
    """åˆ†æå•ä¸ªè¯„ä¼°æ–‡ä»¶"""
    with open(eval_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    entities = data.get('entities', [])
    relations = data.get('relations', [])
    
    # ç»Ÿè®¡å®ä½“è¯„ä¼°ç»“æœ
    entity_stats = {
        'total': len(entities),
        'correct': sum(1 for e in entities if e.get('evaluation') == 'æ­£ç¡®'),
        'incorrect': sum(1 for e in entities if e.get('evaluation') == 'é”™è¯¯'),
        'uncertain': sum(1 for e in entities if e.get('evaluation') == 'ä¸ç¡®å®š'),
        'missing_eval': sum(1 for e in entities if 'evaluation' not in e)
    }
    
    # ç»Ÿè®¡å…³ç³»è¯„ä¼°ç»“æœ
    relation_stats = {
        'total': len(relations),
        'correct': sum(1 for r in relations if r.get('evaluation') == 'æ­£ç¡®'),
        'incorrect': sum(1 for r in relations if r.get('evaluation') == 'é”™è¯¯'),
        'uncertain': sum(1 for r in relations if r.get('evaluation') == 'ä¸ç¡®å®š'),
        'missing_eval': sum(1 for r in relations if 'evaluation' not in r)
    }
    
    # ç»Ÿè®¡å®ä½“ç±»å‹åˆ†å¸ƒï¼ˆæ­£ç¡®vsé”™è¯¯ï¼‰
    entity_type_correct = defaultdict(int)
    entity_type_incorrect = defaultdict(int)
    entity_type_total = defaultdict(int)
    
    for entity in entities:
        etype = entity.get('type', 'unknown')
        evaluation = entity.get('evaluation', 'unknown')
        entity_type_total[etype] += 1
        if evaluation == 'æ­£ç¡®':
            entity_type_correct[etype] += 1
        elif evaluation == 'é”™è¯¯':
            entity_type_incorrect[etype] += 1
    
    # ç»Ÿè®¡å…³ç³»ç±»å‹åˆ†å¸ƒï¼ˆæ­£ç¡®vsé”™è¯¯ï¼‰
    relation_type_correct = defaultdict(int)
    relation_type_incorrect = defaultdict(int)
    relation_type_total = defaultdict(int)
    
    for relation in relations:
        rtype = relation.get('relation', 'unknown')
        evaluation = relation.get('evaluation', 'unknown')
        relation_type_total[rtype] += 1
        if evaluation == 'æ­£ç¡®':
            relation_type_correct[rtype] += 1
        elif evaluation == 'é”™è¯¯':
            relation_type_incorrect[rtype] += 1
    
    return {
        'entity_stats': entity_stats,
        'relation_stats': relation_stats,
        'entity_type_correct': dict(entity_type_correct),
        'entity_type_incorrect': dict(entity_type_incorrect),
        'entity_type_total': dict(entity_type_total),
        'relation_type_correct': dict(relation_type_correct),
        'relation_type_incorrect': dict(relation_type_incorrect),
        'relation_type_total': dict(relation_type_total)
    }

def analyze_model_results(model_name: str, eval_dir: Path) -> dict:
    """åˆ†æå•ä¸ªæ¨¡å‹çš„æ‰€æœ‰è¯„ä¼°ç»“æœ"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š åˆ†æ {model_name} æ¨¡å‹çš„è¯„ä¼°ç»“æœ")
    print(f"{'='*80}")
    
    if not eval_dir.exists():
        print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {eval_dir}")
        return None
    
    eval_files = sorted(eval_dir.glob("*_evaluated.json"))
    
    if not eval_files:
        print(f"âš ï¸ æœªæ‰¾åˆ°è¯„ä¼°æ–‡ä»¶")
        return None
    
    print(f"ğŸ“ æ‰¾åˆ° {len(eval_files)} ä¸ªè¯„ä¼°æ–‡ä»¶")
    
    # æ±‡æ€»ç»Ÿè®¡
    total_entity_stats = {
        'total': 0,
        'correct': 0,
        'incorrect': 0,
        'uncertain': 0,
        'missing_eval': 0
    }
    
    total_relation_stats = {
        'total': 0,
        'correct': 0,
        'incorrect': 0,
        'uncertain': 0,
        'missing_eval': 0
    }
    
    # æ‰€æœ‰å®ä½“ç±»å‹ç»Ÿè®¡
    all_entity_type_correct = defaultdict(int)
    all_entity_type_incorrect = defaultdict(int)
    all_entity_type_total = defaultdict(int)
    
    # æ‰€æœ‰å…³ç³»ç±»å‹ç»Ÿè®¡
    all_relation_type_correct = defaultdict(int)
    all_relation_type_incorrect = defaultdict(int)
    all_relation_type_total = defaultdict(int)
    
    # é€æ–‡ä»¶è¯¦ç»†ç»“æœ
    paper_details = []
    
    for eval_file in eval_files:
        paper_name = eval_file.stem.replace('_evaluated', '')
        
        try:
            file_analysis = analyze_single_file(eval_file)
            
            # æ±‡æ€»å®ä½“ç»Ÿè®¡
            for key in total_entity_stats:
                total_entity_stats[key] += file_analysis['entity_stats'][key]
            
            # æ±‡æ€»å…³ç³»ç»Ÿè®¡
            for key in total_relation_stats:
                total_relation_stats[key] += file_analysis['relation_stats'][key]
            
            # æ±‡æ€»ç±»å‹ç»Ÿè®¡
            for etype, count in file_analysis['entity_type_correct'].items():
                all_entity_type_correct[etype] += count
            for etype, count in file_analysis['entity_type_incorrect'].items():
                all_entity_type_incorrect[etype] += count
            for etype, count in file_analysis['entity_type_total'].items():
                all_entity_type_total[etype] += count
            
            for rtype, count in file_analysis['relation_type_correct'].items():
                all_relation_type_correct[rtype] += count
            for rtype, count in file_analysis['relation_type_incorrect'].items():
                all_relation_type_incorrect[rtype] += count
            for rtype, count in file_analysis['relation_type_total'].items():
                all_relation_type_total[rtype] += count
            
            # è®°å½•è®ºæ–‡è¯¦ç»†ç»“æœ
            entity_accuracy = (file_analysis['entity_stats']['correct'] / 
                             file_analysis['entity_stats']['total'] * 100) if file_analysis['entity_stats']['total'] > 0 else 0
            relation_accuracy = (file_analysis['relation_stats']['correct'] / 
                               file_analysis['relation_stats']['total'] * 100) if file_analysis['relation_stats']['total'] > 0 else 0
            
            paper_details.append({
                'paper': paper_name,
                'entity_total': file_analysis['entity_stats']['total'],
                'entity_correct': file_analysis['entity_stats']['correct'],
                'entity_incorrect': file_analysis['entity_stats']['incorrect'],
                'entity_accuracy': round(entity_accuracy, 2),
                'relation_total': file_analysis['relation_stats']['total'],
                'relation_correct': file_analysis['relation_stats']['correct'],
                'relation_incorrect': file_analysis['relation_stats']['incorrect'],
                'relation_accuracy': round(relation_accuracy, 2)
            })
            
        except Exception as e:
            print(f"âš ï¸ åˆ†ææ–‡ä»¶å¤±è´¥ {eval_file.name}: {e}")
            continue
    
    # è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
    entity_accuracy = (total_entity_stats['correct'] / 
                      total_entity_stats['total'] * 100) if total_entity_stats['total'] > 0 else 0
    relation_accuracy = (total_relation_stats['correct'] / 
                        total_relation_stats['total'] * 100) if total_relation_stats['total'] > 0 else 0
    
    print(f"\nå®ä½“ç»Ÿè®¡:")
    print(f"  - æ€»æ•°: {total_entity_stats['total']}")
    print(f"  - æ­£ç¡®: {total_entity_stats['correct']} ({entity_accuracy:.2f}%)")
    print(f"  - é”™è¯¯: {total_entity_stats['incorrect']}")
    print(f"  - ä¸ç¡®å®š: {total_entity_stats['uncertain']}")
    
    print(f"\nå…³ç³»ç»Ÿè®¡:")
    print(f"  - æ€»æ•°: {total_relation_stats['total']}")
    print(f"  - æ­£ç¡®: {total_relation_stats['correct']} ({relation_accuracy:.2f}%)")
    print(f"  - é”™è¯¯: {total_relation_stats['incorrect']}")
    print(f"  - ä¸ç¡®å®š: {total_relation_stats['uncertain']}")
    
    return {
        'model': model_name,
        'files_analyzed': len(eval_files),
        'entity_stats': total_entity_stats,
        'relation_stats': total_relation_stats,
        'entity_accuracy': round(entity_accuracy, 2),
        'relation_accuracy': round(relation_accuracy, 2),
        'entity_type_correct': dict(all_entity_type_correct),
        'entity_type_incorrect': dict(all_entity_type_incorrect),
        'entity_type_total': dict(all_entity_type_total),
        'relation_type_correct': dict(all_relation_type_correct),
        'relation_type_incorrect': dict(all_relation_type_incorrect),
        'relation_type_total': dict(all_relation_type_total),
        'paper_details': paper_details
    }

def generate_entity_type_accuracy_table(model_results: dict) -> pd.DataFrame:
    """ç”Ÿæˆå®ä½“ç±»å‹å‡†ç¡®ç‡è¡¨æ ¼"""
    rows = []
    
    for etype in sorted(model_results['entity_type_total'].keys()):
        total = model_results['entity_type_total'][etype]
        correct = model_results['entity_type_correct'].get(etype, 0)
        incorrect = model_results['entity_type_incorrect'].get(etype, 0)
        accuracy = (correct / total * 100) if total > 0 else 0
        
        rows.append({
            'å®ä½“ç±»å‹': etype,
            'æ€»æ•°': total,
            'æ­£ç¡®': correct,
            'é”™è¯¯': incorrect,
            'å‡†ç¡®ç‡(%)': round(accuracy, 2)
        })
    
    return pd.DataFrame(rows)

def generate_relation_type_accuracy_table(model_results: dict) -> pd.DataFrame:
    """ç”Ÿæˆå…³ç³»ç±»å‹å‡†ç¡®ç‡è¡¨æ ¼"""
    rows = []
    
    for rtype in sorted(model_results['relation_type_total'].keys()):
        total = model_results['relation_type_total'][rtype]
        correct = model_results['relation_type_correct'].get(rtype, 0)
        incorrect = model_results['relation_type_incorrect'].get(rtype, 0)
        accuracy = (correct / total * 100) if total > 0 else 0
        
        rows.append({
            'å…³ç³»ç±»å‹': rtype,
            'æ€»æ•°': total,
            'æ­£ç¡®': correct,
            'é”™è¯¯': incorrect,
            'å‡†ç¡®ç‡(%)': round(accuracy, 2)
        })
    
    return pd.DataFrame(rows)

# ------------------------------
# ä¸»ç¨‹åº
# ------------------------------
def main():
    print("=" * 80)
    print("ğŸ“ˆ è¯„ä¼°ç»“æœç»Ÿè®¡åˆ†æ")
    print("=" * 80)
    print(f"å®éªŒ: {EXP_ID}")
    print(f"è¯„ä¼°è¾“å…¥ç›®å½•: {EVAL_DIR}")
    print(f"è¾“å‡ºç›®å½•: {ANALYSIS_OUTPUT_DIR}")
    
    # åˆ†æä¸‰ä¸ªæ¨¡å‹
    models_config = [
        ("DeepSeek", DEEPSEEK_EVAL_DIR),
        ("Gemini", GEMINI_EVAL_DIR),
        ("Kimi", KIMI_EVAL_DIR)
    ]
    
    all_model_results = []
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    for model_name, eval_dir in models_config:
        result = analyze_model_results(model_name, eval_dir)
        if result:
            all_model_results.append(result)
            
            # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºå­ç›®å½•
            model_output_dir = ANALYSIS_OUTPUT_DIR / model_name.lower()
            os.makedirs(model_output_dir, exist_ok=True)
            
            # åªä¿å­˜è®ºæ–‡çº§åˆ«è¯¦ç»†ç»“æœåˆ°æ¨¡å‹å­ç›®å½•
            paper_details_df = pd.DataFrame(result['paper_details'])
            paper_details_file = model_output_dir / f"paper_details.csv"
            paper_details_df.to_csv(paper_details_file, index=False, encoding='utf-8-sig')
            print(f"\nâœ… {model_name} è®ºæ–‡è¯¦æƒ…å·²ä¿å­˜: {paper_details_file}")
    
    if not all_model_results:
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¯„ä¼°ç»“æœ")
        return
    
    # ------------------------------
    # ç”Ÿæˆæ±‡æ€» Markdown æŠ¥å‘Šï¼ˆæ”¾åœ¨å¤–å±‚ç›®å½•ï¼‰
    # ------------------------------
    print("\n" + "=" * 80)
    print("ğŸ“‹ ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š")
    print("=" * 80)
    
    # æ±‡æ€»è¡¨æ ¼
    summary_df = pd.DataFrame([
        {
            'æ¨¡å‹': r['model'],
            'è¯„ä¼°æ–‡ä»¶æ•°': r['files_analyzed'],
            'å®ä½“æ€»æ•°': r['entity_stats']['total'],
            'å®ä½“æ­£ç¡®': r['entity_stats']['correct'],
            'å®ä½“é”™è¯¯': r['entity_stats']['incorrect'],
            'å®ä½“å‡†ç¡®ç‡(%)': r['entity_accuracy'],
            'å…³ç³»æ€»æ•°': r['relation_stats']['total'],
            'å…³ç³»æ­£ç¡®': r['relation_stats']['correct'],
            'å…³ç³»é”™è¯¯': r['relation_stats']['incorrect'],
            'å…³ç³»å‡†ç¡®ç‡(%)': r['relation_accuracy']
        }
        for r in all_model_results
    ])
    
    # æ‰“å°æ±‡æ€»
    print(summary_df.to_string(index=False))
    
    # ç”Ÿæˆ Markdown æŠ¥å‘Šï¼ˆæ”¾åœ¨å¤–å±‚ï¼‰
    report_file = ANALYSIS_OUTPUT_DIR / f"evaluation_summary.md"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# RAG æ¨¡å‹æŠ½å–ç»“æœè¯„ä¼°åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## 1. æ¨¡å‹å¯¹æ¯”æ±‡æ€»\n\n")
        # æ‰‹åŠ¨ç”Ÿæˆ Markdown è¡¨æ ¼
        headers = summary_df.columns.tolist()
        f.write("| " + " | ".join(headers) + " |\n")
        f.write("|" + "|".join(["---"] * len(headers)) + "|\n")
        for _, row in summary_df.iterrows():
            f.write("| " + " | ".join(str(v) for v in row.values) + " |\n")
        f.write("\n")
        
        f.write("## 2. å…³é”®å‘ç°\n\n")
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_entity = max(all_model_results, key=lambda x: x['entity_accuracy'])
        best_relation = max(all_model_results, key=lambda x: x['relation_accuracy'])
        
        f.write(f"- **å®ä½“æŠ½å–æœ€ä¼˜**: {best_entity['model']} ({best_entity['entity_accuracy']}%)\n")
        f.write(f"- **å…³ç³»æŠ½å–æœ€ä¼˜**: {best_relation['model']} ({best_relation['relation_accuracy']}%)\n\n")
        
        f.write("## 3. å„æ¨¡å‹è¯¦ç»†åˆ†æ\n\n")
        
        for model_result in all_model_results:
            model_name = model_result['model']
            f.write(f"### {model_name}\n\n")
            f.write(f"- å®ä½“å‡†ç¡®ç‡: {model_result['entity_accuracy']}%\n")
            f.write(f"- å…³ç³»å‡†ç¡®ç‡: {model_result['relation_accuracy']}%\n")
            f.write(f"- å®ä½“æ€»æ•°: {model_result['entity_stats']['total']}\n")
            f.write(f"- å…³ç³»æ€»æ•°: {model_result['relation_stats']['total']}\n")
            f.write(f"- è¯¦ç»†æ•°æ®: `{model_name.lower()}/paper_details.csv`\n\n")
        
        f.write("## 4. æ–‡ä»¶è¯´æ˜\n\n")
        f.write("```\n")
        f.write("evaluation_results/\n")
        f.write("â”œâ”€â”€ evaluation_summary.md     # æœ¬æ–‡ä»¶ï¼ˆæ±‡æ€»æŠ¥å‘Šï¼‰\n")
        f.write("â”œâ”€â”€ deepseek/\n")
        f.write("â”‚   â””â”€â”€ paper_details.csv     # DeepSeek æ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†ç»“æœ\n")
        f.write("â”œâ”€â”€ gemini/\n")
        f.write("â”‚   â””â”€â”€ paper_details.csv     # Gemini æ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†ç»“æœ\n")
        f.write("â””â”€â”€ kimi/\n")
        f.write("    â””â”€â”€ paper_details.csv     # Kimi æ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†ç»“æœ\n")
        f.write("```\n\n")
        
        f.write("## 5. æ”¹è¿›å»ºè®®\n\n")
        f.write("1. åˆ†æé”™è¯¯å®ä½“å’Œå…³ç³»çš„å…·ä½“ç±»å‹ï¼Œé’ˆå¯¹æ€§ä¼˜åŒ– Prompt\n")
        f.write("2. å¯¹å‡†ç¡®ç‡è¾ƒä½çš„å®ä½“/å…³ç³»ç±»å‹å¢åŠ æ›´å¤š few-shot ç¤ºä¾‹\n")
        f.write("3. äººå·¥å¤æ ¸é”™è¯¯æ¡ˆä¾‹ï¼Œè°ƒæ•´ schema å®šä¹‰\n")
        f.write("4. è€ƒè™‘ä½¿ç”¨é›†æˆæ–¹æ³•ç»“åˆå¤šä¸ªæ¨¡å‹çš„ä¼˜åŠ¿\n")
    
    print(f"\nâœ… æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    # æ‰“å°ç»Ÿè®¡äº®ç‚¹
    print("\n" + "=" * 80)
    print("ğŸ¯ ç»Ÿè®¡äº®ç‚¹")
    print("=" * 80)
    
    # å®ä½“å‡†ç¡®ç‡å¯¹æ¯”
    entity_accuracies = [(r['model'], r['entity_accuracy']) for r in all_model_results]
    entity_accuracies.sort(key=lambda x: x[1], reverse=True)
    
    print("\nå®ä½“å‡†ç¡®ç‡æ’å:")
    for rank, (model, acc) in enumerate(entity_accuracies, 1):
        print(f"  {rank}. {model}: {acc}%")
    
    # å…³ç³»å‡†ç¡®ç‡å¯¹æ¯”
    relation_accuracies = [(r['model'], r['relation_accuracy']) for r in all_model_results]
    relation_accuracies.sort(key=lambda x: x[1], reverse=True)
    
    print("\nå…³ç³»å‡†ç¡®ç‡æ’å:")
    for rank, (model, acc) in enumerate(relation_accuracies, 1):
        print(f"  {rank}. {model}: {acc}%")
    
    print("\n" + "=" * 80)
    print("âœ… åˆ†æå®Œæˆ!")
    print("=" * 80)
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {ANALYSIS_OUTPUT_DIR}")
    print(f"   - æ±‡æ€»æŠ¥å‘Š: evaluation_summary.md")
    print(f"   - DeepSeekè¯¦æƒ…: deepseek/paper_details.csv")
    print(f"   - Geminiè¯¦æƒ…: gemini/paper_details.csv")
    print(f"   - Kimiè¯¦æƒ…: kimi/paper_details.csv")

if __name__ == "__main__":
    main()
