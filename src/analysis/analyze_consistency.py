# -*- coding: utf-8 -*-
"""
ä¸€è‡´æ€§åˆ†æè„šæœ¬

è¾“å…¥ç›®å½•ï¼ˆå›ºå®šçº¦å®šï¼‰ï¼š
  E:/langchain/outputs/extractions/consistency/
  â”œâ”€â”€ 1/
  â”‚   â”œâ”€â”€ deepseek_rag/
  â”‚   â”œâ”€â”€ gemini_rag/
  â”‚   â””â”€â”€ kimi_rag/
  â”œâ”€â”€ 2/
  â”‚   â”œâ”€â”€ deepseek_rag/
  â”‚   â”œâ”€â”€ gemini_rag/
  â”‚   â””â”€â”€ kimi_rag/
  â””â”€â”€ 3/
      â”œâ”€â”€ deepseek_rag/
      â””â”€â”€ gemini_rag/

è¾“å‡ºç›®å½•ï¼ˆä¸å…¶å®ƒåˆ†æè„šæœ¬ä¸€è‡´çš„æç®€é£æ ¼ï¼‰ï¼š
  E:/langchain/outputs/analysis/consistency/
  â”œâ”€â”€ consistency_summary.md
  â”œâ”€â”€ deepseek/paper_consistency.csv
  â”œâ”€â”€ gemini/paper_consistency.csv
  â””â”€â”€ kimi/paper_consistency.csv

æŒ‡æ ‡å®šä¹‰ï¼ˆåŸºç¡€ç‰ˆï¼‰ï¼š
  - å®ä½“é›†åˆä¸€è‡´æ€§ï¼ˆJaccard å¹³å‡ï¼‰ï¼šå¯¹åŒä¸€è®ºæ–‡è·¨å¤šæ¬¡è¿è¡Œçš„å®ä½“é›†åˆåšæˆå¯¹ Jaccard ç›¸ä¼¼åº¦ï¼Œå–å¹³å‡
  - å…³ç³»é›†åˆä¸€è‡´æ€§ï¼ˆJaccard å¹³å‡ï¼‰ï¼šåŒä¸Š
  - è§„æ¨¡ç¨³å®šæ€§ï¼ˆCVï¼‰ï¼šå®ä½“æ•°ã€å…³ç³»æ•°åœ¨å¤šæ¬¡è¿è¡Œé—´çš„å˜å¼‚ç³»æ•°ï¼ˆstd/meanï¼‰

å®ä½“é”®ä¸å…³ç³»é”®ï¼š
  - å®ä½“é”®ï¼šnormalize(name) + '::' + normalize(type)
  - å…³ç³»é”®ï¼šnormalize(src_name+'::'+src_type) + '->' + normalize(relation_type) + '->' + normalize(tgt_name+'::'+tgt_type)

å¥å£®æ€§ï¼š
  - JSON ä¸­é”®åå¯èƒ½ä¸å®Œå…¨ä¸€è‡´ï¼Œåšå…¼å®¹æå–ï¼›ç¼ºå¤±åˆ™è·³è¿‡ã€‚
  - å°‘äº 2 æ¬¡è¿è¡Œæ—¶ï¼ŒJaccard/CV è®°ä¸º NAã€‚
"""

from __future__ import annotations

import os
import json
import math
from pathlib import Path
from itertools import combinations
from collections import defaultdict, Counter
from typing import Dict, List, Set, Tuple, Any

import pandas as pd

CONSISTENCY_BASE = Path("E:/langchain/outputs/extractions/consistency")
OUTPUT_DIR = Path("E:/langchain/outputs/analysis/consistency")

MODEL_FOLDERS = {
    "deepseek": "deepseek_rag",
    "gemini": "gemini_rag",
    "kimi": "kimi_rag",
}


def norm_text(x: Any) -> str:
    if x is None:
        return ""
    s = str(x).strip().lower()
    # å¯æ‹“å±•ï¼šå…¨è§’åŠè§’ã€ç‰¹æ®Šç©ºæ ¼
    return " ".join(s.split())


def entity_key(ent: dict) -> str:
    name = ent.get("name") or ent.get("entity") or ent.get("text")
    etype = ent.get("type") or ent.get("entity_type") or ent.get("category")
    return f"{norm_text(name)}::{norm_text(etype)}"


def relation_key(rel: dict) -> str:
    # å…¼å®¹ä¸åŒå­—æ®µå‘½å
    src_name = rel.get("source_name") or rel.get("source") or rel.get("from")
    src_type = rel.get("source_type") or rel.get("from_type") or rel.get("sourceEntityType")
    tgt_name = rel.get("target_name") or rel.get("target") or rel.get("to")
    tgt_type = rel.get("target_type") or rel.get("to_type") or rel.get("targetEntityType")
    rtype = rel.get("type") or rel.get("relation") or rel.get("relation_type")
    return f"{norm_text(src_name)}::{norm_text(src_type)}->{norm_text(rtype)}->{norm_text(tgt_name)}::{norm_text(tgt_type)}"


def load_json_safely(p: Path) -> dict | None:
    try:
        with open(p, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None


def extract_sets_from_file(p: Path) -> Tuple[Set[str], Set[str], int, int]:
    data = load_json_safely(p)
    if not isinstance(data, dict):
        return set(), set(), 0, 0
    ents = data.get("entities") or []
    rels = data.get("relations") or []
    ent_set = set()
    for e in ents:
        try:
            ent_set.add(entity_key(e))
        except Exception:
            continue
    rel_set = set()
    for r in rels:
        try:
            rel_set.add(relation_key(r))
        except Exception:
            continue
    return ent_set, rel_set, len(ents) if isinstance(ents, list) else 0, len(rels) if isinstance(rels, list) else 0


def jaccard(a: Set[str], b: Set[str]) -> float:
    if not a and not b:
        return 1.0
    if not a or not b:
        return 0.0
    inter = len(a & b)
    union = len(a | b)
    if union == 0:
        return 1.0
    return inter / union


def coeff_variation(values: List[float]) -> float | None:
    if len(values) < 2:
        return None
    mean = sum(values) / len(values)
    if mean == 0:
        return None
    var = sum((v - mean) ** 2 for v in values) / (len(values) - 1)
    std = math.sqrt(var)
    return std / mean


def scan_run_files(run_dir: Path, model_folder: str) -> Dict[str, Path]:
    """æ‰«æå•æ¬¡è¿è¡Œä¸­æŸæ¨¡å‹çš„æ‰€æœ‰è®ºæ–‡ç»“æœæ–‡ä»¶ï¼Œè¿”å› {paper_id: path}ã€‚
    paper_id ç”¨æ–‡ä»¶åï¼ˆå»æ‰©å±•åï¼‰ã€‚é€’å½’ rglob ä»¥å…¼å®¹å­ç›®å½•ã€‚
    """
    base = run_dir / model_folder
    results = {}
    if not base.exists():
        return results
    for p in base.rglob("*.json"):
        # è¿‡æ»¤æ‰æ˜æ˜¾çš„æ—¥å¿—æ–‡ä»¶ï¼ˆåå­—åŒ…å« logï¼‰
        if "log" in p.name.lower():
            continue
        paper = p.stem
        results[paper] = p
    return results


def analyze_model(model_key: str, runs: List[Path]) -> pd.DataFrame:
    model_folder = MODEL_FOLDERS[model_key]

    # æ”¶é›†æ¯æ¬¡è¿è¡Œçš„æ–‡ä»¶æ˜ å°„
    run_maps: List[Dict[str, Path]] = [scan_run_files(r, model_folder) for r in runs]

    # æ±‡æ€»æ‰€æœ‰è®ºæ–‡ID
    all_papers: Set[str] = set()
    for m in run_maps:
        all_papers.update(m.keys())

    rows = []
    for paper in sorted(all_papers):
        ent_sets: List[Set[str]] = []
        rel_sets: List[Set[str]] = []
        ent_counts: List[int] = []
        rel_counts: List[int] = []
        used_runs = 0

        for m in run_maps:
            path = m.get(paper)
            if not path:
                continue
            ent_set, rel_set, ec, rc = extract_sets_from_file(path)
            ent_sets.append(ent_set)
            rel_sets.append(rel_set)
            ent_counts.append(ec)
            rel_counts.append(rc)
            used_runs += 1

        # è®¡ç®—å¹³å‡ pairwise Jaccard
        def avg_pairwise_jacc(sets: List[Set[str]]) -> float | None:
            if len(sets) < 2:
                return None
            vals = []
            for a, b in combinations(sets, 2):
                vals.append(jaccard(a, b))
            return sum(vals) / len(vals) if vals else None

        ent_j = avg_pairwise_jacc(ent_sets)
        rel_j = avg_pairwise_jacc(rel_sets)
        ecv = coeff_variation(ent_counts)
        rcv = coeff_variation(rel_counts)

        rows.append({
            "paper": paper,
            "runs": used_runs,
            "entity_jaccard": None if ent_j is None else round(ent_j, 4),
            "relation_jaccard": None if rel_j is None else round(rel_j, 4),
            "entities_cv": None if ecv is None else round(ecv, 4),
            "relations_cv": None if rcv is None else round(rcv, 4),
            "avg_entities": round(sum(ent_counts) / used_runs, 2) if used_runs else None,
            "avg_relations": round(sum(rel_counts) / used_runs, 2) if used_runs else None,
        })

    return pd.DataFrame(rows)


def main():
    print("=" * 80)
    print("ğŸ” æŠ½å–ç»“æœä¸€è‡´æ€§åˆ†æ")
    print("=" * 80)

    # æ”¶é›†æœ‰æ•ˆçš„è¿è¡Œç›®å½•ï¼ˆæ•°å­—å‘½åï¼‰
    if not CONSISTENCY_BASE.exists():
        raise SystemExit(f"æœªæ‰¾åˆ°ç›®å½•: {CONSISTENCY_BASE}")

    runs = [d for d in CONSISTENCY_BASE.iterdir() if d.is_dir() and d.name.isdigit()]
    runs.sort(key=lambda p: int(p.name))
    print(f"å‘ç°è¿è¡Œæ¬¡æ•°: {len(runs)} -> {[p.name for p in runs]}")

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    model_summaries = []
    for model_key in ["deepseek", "gemini", "kimi"]:
        df = analyze_model(model_key, runs)
        model_dir = OUTPUT_DIR / model_key
        model_dir.mkdir(parents=True, exist_ok=True)
        out_csv = model_dir / "paper_consistency.csv"
        df.to_csv(out_csv, index=False, encoding="utf-8-sig")
        print(f"   âœ… {model_key} ç»“æœå·²ä¿å­˜: {out_csv}")

        # ç»Ÿè®¡æ±‡æ€»ï¼ˆå¿½ç•¥ NAï¼‰
        def safe_mean(series: pd.Series) -> float | None:
            s = series.dropna()
            return round(s.mean(), 4) if len(s) else None

        summary = {
            "æ¨¡å‹": model_key,
            "è®ºæ–‡æ•°": len(df),
            "å¹³å‡å®ä½“Jaccard": safe_mean(df["entity_jaccard"]) if not df.empty else None,
            "å¹³å‡å…³ç³»Jaccard": safe_mean(df["relation_jaccard"]) if not df.empty else None,
            "å®ä½“æ•°CV(å‡å€¼)": safe_mean(df["entities_cv"]) if not df.empty else None,
            "å…³ç³»æ•°CV(å‡å€¼)": safe_mean(df["relations_cv"]) if not df.empty else None,
        }
        model_summaries.append(summary)

    # ç”Ÿæˆæ±‡æ€» Markdown
    md_file = OUTPUT_DIR / "consistency_summary.md"
    with open(md_file, "w", encoding="utf-8") as f:
        f.write("# æŠ½å–ç»“æœä¸€è‡´æ€§åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"è¿è¡Œç›®å½•: `{CONSISTENCY_BASE}`\n\n")
        f.write(f"å…±å‘ç° {len(runs)} æ¬¡è¿è¡Œ: {', '.join(p.name for p in runs)}\n\n")

        if model_summaries:
            f.write("## æ¨¡å‹çº§æ±‡æ€»\n\n")
            headers = ["æ¨¡å‹", "è®ºæ–‡æ•°", "å¹³å‡å®ä½“Jaccard", "å¹³å‡å…³ç³»Jaccard", "å®ä½“æ•°CV(å‡å€¼)", "å…³ç³»æ•°CV(å‡å€¼)"]
            f.write("| " + " | ".join(headers) + " |\n")
            f.write("|" + "|".join(["---"] * len(headers)) + "|\n")
            for s in model_summaries:
                row = [
                    s["æ¨¡å‹"],
                    s["è®ºæ–‡æ•°"],
                    s["å¹³å‡å®ä½“Jaccard"],
                    s["å¹³å‡å…³ç³»Jaccard"],
                    s["å®ä½“æ•°CV(å‡å€¼)"],
                    s["å…³ç³»æ•°CV(å‡å€¼)"],
                ]
                f.write("| " + " | ".join("" if v is None else str(v) for v in row) + " |\n")
            f.write("\n")

        f.write("## æ–‡ä»¶ç»“æ„\n\n")
        f.write("```)\n")
        f.write("outputs/analysis/consistency/\n")
        f.write("â”œâ”€â”€ consistency_summary.md\n")
        f.write("â”œâ”€â”€ deepseek/paper_consistency.csv\n")
        f.write("â”œâ”€â”€ gemini/paper_consistency.csv\n")
        f.write("â””â”€â”€ kimi/paper_consistency.csv\n")
        f.write("```)\n\n")

        f.write("## æŒ‡æ ‡è¯´æ˜\n\n")
        f.write("- å®ä½“/å…³ç³» Jaccard: è·¨è¿è¡Œçš„é›†åˆç›¸ä¼¼åº¦ï¼ˆæˆå¯¹å¹³å‡ï¼‰ï¼Œè¶Šé«˜è¶Šç¨³å®š\n")
        f.write("- å®ä½“æ•°/å…³ç³»æ•° CV: è§„æ¨¡æ³¢åŠ¨ï¼ˆstd/meanï¼‰ï¼Œè¶Šä½è¶Šç¨³å®š\n")
        f.write("- runs åˆ—ç¤ºè¯¥è®ºæ–‡å®é™…å‚ä¸ç»Ÿè®¡çš„è¿è¡Œæ¬¡æ•°ï¼ˆç¼ºå¤±ä¼šè‡ªåŠ¨è·³è¿‡ï¼‰\n")

    print(f"\nâœ… æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {md_file}")


if __name__ == "__main__":
    main()
