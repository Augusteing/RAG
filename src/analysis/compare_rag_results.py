# -*- coding: utf-8 -*-
"""
RAG æ¨¡å‹ç»“æœå¯¹æ¯”åˆ†æè„šæœ¬
ç»Ÿè®¡ DeepSeekã€Geminiã€Kimi ä¸‰ä¸ªæ¨¡å‹çš„æŠ½å–ç»“æœ
ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼å’Œå¯è§†åŒ–å›¾è¡¨
"""
import os
import json
import argparse
from pathlib import Path
from collections import defaultdict
import pandas as pd
from datetime import datetime

# ------------------------------
# è·¯å¾„é…ç½®
# ------------------------------
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent

# å®éªŒé€‰æ‹©ï¼šå‘½ä»¤è¡Œä¼˜å…ˆï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡ï¼Œé»˜è®¤ exp02
_parser = argparse.ArgumentParser(add_help=False)
_parser.add_argument("--exp", dest="exp_id", choices=["exp02", "exp03"], default=None)
_parser.add_argument("--exp02", dest="exp02", action="store_true")
_parser.add_argument("--exp03", dest="exp03", action="store_true")
_parser.add_argument("--03", dest="exp03_short", action="store_true")
_args, _unknown = _parser.parse_known_args()
_env_exp = os.getenv("EXP_ID")
if _args.exp_id in {"exp02", "exp03"}:
    EXP_ID = _args.exp_id
elif _args.exp03 or _args.exp03_short:
    EXP_ID = "exp03"
elif _args.exp02:
    EXP_ID = "exp02"
elif _env_exp in {"exp02", "exp03"}:
    EXP_ID = _env_exp
else:
    EXP_ID = "exp02"

# ä¸‰ä¸ªæ¨¡å‹çš„è¾“å‡ºç›®å½•ï¼ˆæŒ‰å®éªŒåˆ†æµï¼‰
DEEPSEEK_DIR = Path(f"E:/langchain/outputs/extractions/{EXP_ID}/deepseek_rag")
GEMINI_DIR = Path(f"E:/langchain/outputs/extractions/{EXP_ID}/gemini_rag")
KIMI_DIR = Path(f"E:/langchain/outputs/extractions/{EXP_ID}/kimi_rag")

# æ—¥å¿—ç›®å½•ï¼ˆæŒ‰å®éªŒåˆ†æµï¼‰
DEEPSEEK_LOG = Path(f"E:/langchain/outputs/logs/{EXP_ID}/deepseek")
GEMINI_LOG = Path(f"E:/langchain/outputs/logs/{EXP_ID}/gemini")
KIMI_LOG = Path(f"E:/langchain/outputs/logs/{EXP_ID}/kimi")

# è¾“å‡ºç›®å½•ï¼ˆæŒ‰å®éªŒåˆ†æµï¼‰
OUTPUT_DIR = Path(f"E:/langchain/outputs/analysis/statistics/{EXP_ID}")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ------------------------------
# æ•°æ®ç»Ÿè®¡å‡½æ•°
# ------------------------------
def count_json_files(directory: Path) -> dict:
    """ç»Ÿè®¡ç›®å½•ä¸‹çš„JSONæ–‡ä»¶æ•°é‡å’ŒåŸºæœ¬ä¿¡æ¯"""
    if not directory.exists():
        return {
            "total": 0,
            "success": 0,
            "failed": 0,
            "files": []
        }
    
    json_files = list(directory.glob("*.json"))
    error_files = list(directory.glob("*.error.txt"))
    
    return {
        "total": len(json_files) + len(error_files),
        "success": len(json_files),
        "failed": len(error_files),
        "files": [f.name for f in json_files]
    }

def analyze_json_content(file_path: Path) -> dict:
    """åˆ†æå•ä¸ªJSONæ–‡ä»¶çš„å†…å®¹"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        entities = data.get("entities", [])
        relations = data.get("relations", [])
        
        # ç»Ÿè®¡å®ä½“ç±»å‹åˆ†å¸ƒ
        entity_types = defaultdict(int)
        for entity in entities:
            entity_type = entity.get("type", "unknown")
            entity_types[entity_type] += 1
        
        # ç»Ÿè®¡å…³ç³»ç±»å‹åˆ†å¸ƒ
        relation_types = defaultdict(int)
        for relation in relations:
            relation_type = relation.get("relation", "unknown")
            relation_types[relation_type] += 1
        
        return {
            "entity_count": len(entities),
            "relation_count": len(relations),
            "entity_type_count": len(entity_types),  # å®ä½“ç±»å‹æ•°é‡
            "relation_type_count": len(relation_types),  # å…³ç³»ç±»å‹æ•°é‡
            "entity_types": dict(entity_types),
            "relation_types": dict(relation_types)
        }
    except Exception as e:
        print(f"âš ï¸ åˆ†ææ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
        return {
            "entity_count": 0,
            "relation_count": 0,
            "entity_type_count": 0,
            "relation_type_count": 0,
            "entity_types": {},
            "relation_types": {}
        }

def analyze_model_results(model_name: str, json_dir: Path, log_dir: Path) -> dict:
    """åˆ†æå•ä¸ªæ¨¡å‹çš„æ‰€æœ‰ç»“æœ"""
    print(f"\nğŸ“Š åˆ†æ {model_name} æ¨¡å‹ç»“æœ...")
    
    # åŸºæœ¬ç»Ÿè®¡
    file_stats = count_json_files(json_dir)
    print(f"   - æˆåŠŸ: {file_stats['success']} ç¯‡")
    print(f"   - å¤±è´¥: {file_stats['failed']} ç¯‡")
    
    # è¯¦ç»†åˆ†ææ¯ä¸ªJSONæ–‡ä»¶
    detailed_results = []
    total_entities = 0
    total_relations = 0
    total_entity_types = 0
    total_relation_types = 0
    
    # ç”¨äºç»Ÿè®¡æ‰€æœ‰è®ºæ–‡ä¸­çš„ç±»å‹ï¼ˆå»é‡ï¼‰
    all_entity_types = set()
    all_relation_types = set()
    
    for json_file in json_dir.glob("*.json"):
        paper_name = json_file.stem
        analysis = analyze_json_content(json_file)
        
        detailed_results.append({
            "paper": paper_name,
            "entities": analysis["entity_count"],
            "relations": analysis["relation_count"],
            "entity_types": analysis["entity_type_count"],
            "relation_types": analysis["relation_type_count"]
        })
        
        total_entities += analysis["entity_count"]
        total_relations += analysis["relation_count"]
        total_entity_types += analysis["entity_type_count"]
        total_relation_types += analysis["relation_type_count"]
        
        # æ”¶é›†æ‰€æœ‰ç±»å‹
        all_entity_types.update(analysis["entity_types"].keys())
        all_relation_types.update(analysis["relation_types"].keys())
    
    # è®¡ç®—å¹³å‡å€¼
    success_count = file_stats['success']
    avg_entities = total_entities / success_count if success_count > 0 else 0
    avg_relations = total_relations / success_count if success_count > 0 else 0
    avg_entity_types = total_entity_types / success_count if success_count > 0 else 0
    avg_relation_types = total_relation_types / success_count if success_count > 0 else 0
    
    return {
        "model": model_name,
        "total_papers": file_stats['total'],
        "success": file_stats['success'],
        "failed": file_stats['failed'],
        "total_entities": total_entities,
        "total_relations": total_relations,
        "avg_entities": round(avg_entities, 2),
        "avg_relations": round(avg_relations, 2),
        "total_entity_types": total_entity_types,
        "total_relation_types": total_relation_types,
        "avg_entity_types": round(avg_entity_types, 2),
        "avg_relation_types": round(avg_relation_types, 2),
        "unique_entity_types": len(all_entity_types),
        "unique_relation_types": len(all_relation_types),
        "detailed_results": detailed_results
    }

# ------------------------------
# ä¸»ç¨‹åº
# ------------------------------
def main():
    print("=" * 80)
    print("ğŸ”¬ RAG æ¨¡å‹æŠ½å–ç»“æœç»Ÿè®¡åˆ†æ")
    print("=" * 80)
    
    # åˆ†æä¸‰ä¸ªæ¨¡å‹
    models_config = [
        ("DeepSeek", DEEPSEEK_DIR, DEEPSEEK_LOG),
        ("Gemini", GEMINI_DIR, GEMINI_LOG),
        ("Kimi", KIMI_DIR, KIMI_LOG)
    ]
    
    all_results = []
    
    for model_name, json_dir, log_dir in models_config:
        result = analyze_model_results(model_name, json_dir, log_dir)
        all_results.append(result)
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºå­ç›®å½•å¹¶ä¿å­˜è®ºæ–‡ç»Ÿè®¡
        model_output_dir = OUTPUT_DIR / model_name.lower()
        os.makedirs(model_output_dir, exist_ok=True)
        
        paper_stats_df = pd.DataFrame(result["detailed_results"])
        paper_stats_file = model_output_dir / "paper_statistics.csv"
        paper_stats_df.to_csv(paper_stats_file, index=False, encoding='utf-8-sig')
        print(f"   âœ… å·²ä¿å­˜: {paper_stats_file}")
    
    # ------------------------------
    # ç”Ÿæˆæ±‡æ€»è¡¨æ ¼
    # ------------------------------
    print("\n" + "=" * 80)
    print("ğŸ“‹ æ¨¡å‹å¯¹æ¯”æ±‡æ€»")
    print("=" * 80)
    
    summary_df = pd.DataFrame([
        {
            "æ¨¡å‹": r["model"],
            "æˆåŠŸè®ºæ–‡æ•°": r["success"],
            "å¤±è´¥è®ºæ–‡æ•°": r["failed"],
            "æ€»å®ä½“æ•°": r["total_entities"],
            "æ€»å…³ç³»æ•°": r["total_relations"],
            "å¹³å‡å®ä½“æ•°": r["avg_entities"],
            "å¹³å‡å…³ç³»æ•°": r["avg_relations"],
            "å¹³å‡å®ä½“ç±»å‹æ•°": r["avg_entity_types"],
            "å¹³å‡å…³ç³»ç±»å‹æ•°": r["avg_relation_types"],
            "æ€»å®ä½“ç±»å‹æ•°": r["unique_entity_types"],
            "æ€»å…³ç³»ç±»å‹æ•°": r["unique_relation_types"]
        }
        for r in all_results
    ])
    
    print(summary_df.to_string(index=False))
    
    # ------------------------------
    # ç”Ÿæˆæ±‡æ€» Markdown æŠ¥å‘Šï¼ˆæ”¾åœ¨å¤–å±‚ï¼‰
    # ------------------------------
    report_file = OUTPUT_DIR / "extraction_summary.md"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# RAG æ¨¡å‹æŠ½å–ç»“æœç»Ÿè®¡æŠ¥å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## 1. æ¨¡å‹å¯¹æ¯”æ±‡æ€»\n\n")
        # æ‰‹åŠ¨ç”Ÿæˆ Markdown è¡¨æ ¼
        headers = summary_df.columns.tolist()
        f.write("| " + " | ".join(headers) + " |\n")
        f.write("|" + "|".join(["---"] * len(headers)) + "|\n")
        for _, row in summary_df.iterrows():
            f.write("| " + " | ".join(str(v) for v in row.values) + " |\n")
        f.write("\n")
        
        f.write("## 2. å…³é”®å‘ç°\n\n")
        
        best_entities = max(all_results, key=lambda x: x["avg_entities"])
        best_relations = max(all_results, key=lambda x: x["avg_relations"])
        best_entity_types = max(all_results, key=lambda x: x["avg_entity_types"])
        best_relation_types = max(all_results, key=lambda x: x["avg_relation_types"])
        most_diverse_entity = max(all_results, key=lambda x: x["unique_entity_types"])
        most_diverse_relation = max(all_results, key=lambda x: x["unique_relation_types"])
        
        f.write(f"- **æœ€å¤šå¹³å‡å®ä½“æ•°**: {best_entities['model']} ({best_entities['avg_entities']} ä¸ª)\n")
        f.write(f"- **æœ€å¤šå¹³å‡å…³ç³»æ•°**: {best_relations['model']} ({best_relations['avg_relations']} ä¸ª)\n")
        f.write(f"- **æœ€å¤šå¹³å‡å®ä½“ç±»å‹æ•°**: {best_entity_types['model']} ({best_entity_types['avg_entity_types']} ç§)\n")
        f.write(f"- **æœ€å¤šå¹³å‡å…³ç³»ç±»å‹æ•°**: {best_relation_types['model']} ({best_relation_types['avg_relation_types']} ç§)\n")
        f.write(f"- **æœ€ä¸°å¯Œå®ä½“ç±»å‹**: {most_diverse_entity['model']} (å…± {most_diverse_entity['unique_entity_types']} ç§ä¸åŒç±»å‹)\n")
        f.write(f"- **æœ€ä¸°å¯Œå…³ç³»ç±»å‹**: {most_diverse_relation['model']} (å…± {most_diverse_relation['unique_relation_types']} ç§ä¸åŒç±»å‹)\n\n")
        
        f.write("## 3. å„æ¨¡å‹è¯¦ç»†æ•°æ®\n\n")
        
        for r in all_results:
            model_name = r['model']
            f.write(f"### {model_name}\n\n")
            f.write(f"- æˆåŠŸè®ºæ–‡æ•°: {r['success']}\n")
            f.write(f"- æ€»å®ä½“æ•°: {r['total_entities']}\n")
            f.write(f"- æ€»å…³ç³»æ•°: {r['total_relations']}\n")
            f.write(f"- å¹³å‡å®ä½“æ•°: {r['avg_entities']}\n")
            f.write(f"- å¹³å‡å…³ç³»æ•°: {r['avg_relations']}\n")
            f.write(f"- å¹³å‡å®ä½“ç±»å‹æ•°: {r['avg_entity_types']}\n")
            f.write(f"- å¹³å‡å…³ç³»ç±»å‹æ•°: {r['avg_relation_types']}\n")
            f.write(f"- æ€»å®ä½“ç±»å‹æ•°ï¼ˆå»é‡ï¼‰: {r['unique_entity_types']}\n")
            f.write(f"- æ€»å…³ç³»ç±»å‹æ•°ï¼ˆå»é‡ï¼‰: {r['unique_relation_types']}\n")
            f.write(f"- è¯¦ç»†æ•°æ®: `{model_name.lower()}/paper_statistics.csv`\n\n")
        
        f.write("## 4. æ–‡ä»¶è¯´æ˜\n\n")
        f.write("```\n")
        f.write("statistics/\n")
        f.write("â”œâ”€â”€ extraction_summary.md         # æœ¬æ–‡ä»¶ï¼ˆæ±‡æ€»æŠ¥å‘Šï¼‰\n")
        f.write("â”œâ”€â”€ deepseek/\n")
        f.write("â”‚   â””â”€â”€ paper_statistics.csv     # DeepSeek æ¯ç¯‡è®ºæ–‡çš„ç»Ÿè®¡æ•°æ®\n")
        f.write("â”œâ”€â”€ gemini/\n")
        f.write("â”‚   â””â”€â”€ paper_statistics.csv     # Gemini æ¯ç¯‡è®ºæ–‡çš„ç»Ÿè®¡æ•°æ®\n")
        f.write("â””â”€â”€ kimi/\n")
        f.write("    â””â”€â”€ paper_statistics.csv     # Kimi æ¯ç¯‡è®ºæ–‡çš„ç»Ÿè®¡æ•°æ®\n")
        f.write("```\n\n")
        
        f.write("## 5. CSV æ–‡ä»¶åˆ—è¯´æ˜\n\n")
        f.write("- **paper**: è®ºæ–‡åç§°\n")
        f.write("- **entities**: å®ä½“æ•°é‡\n")
        f.write("- **relations**: å…³ç³»æ•°é‡\n")
        f.write("- **entity_types**: å®ä½“ç±»å‹æ•°é‡ï¼ˆå»é‡ï¼‰\n")
        f.write("- **relation_types**: å…³ç³»ç±»å‹æ•°é‡ï¼ˆå»é‡ï¼‰\n")
    
    print(f"\nâœ… æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    # æ‰“å°ç»Ÿè®¡äº®ç‚¹
    print("\n" + "=" * 80)
    print("ğŸ¯ ç»Ÿè®¡äº®ç‚¹")
    print("=" * 80)
    
    entity_ranking = sorted(all_results, key=lambda x: x["avg_entities"], reverse=True)
    relation_ranking = sorted(all_results, key=lambda x: x["avg_relations"], reverse=True)
    entity_type_ranking = sorted(all_results, key=lambda x: x["avg_entity_types"], reverse=True)
    relation_type_ranking = sorted(all_results, key=lambda x: x["avg_relation_types"], reverse=True)
    
    print("\nå¹³å‡å®ä½“æ•°æ’å:")
    for rank, r in enumerate(entity_ranking, 1):
        print(f"  {rank}. {r['model']}: {r['avg_entities']} ä¸ª")
    
    print("\nå¹³å‡å…³ç³»æ•°æ’å:")
    for rank, r in enumerate(relation_ranking, 1):
        print(f"  {rank}. {r['model']}: {r['avg_relations']} ä¸ª")
    
    print("\nå¹³å‡å®ä½“ç±»å‹æ•°æ’å:")
    for rank, r in enumerate(entity_type_ranking, 1):
        print(f"  {rank}. {r['model']}: {r['avg_entity_types']} ç§ (æ€»å…± {r['unique_entity_types']} ç§ä¸åŒç±»å‹)")
    
    print("\nå¹³å‡å…³ç³»ç±»å‹æ•°æ’å:")
    for rank, r in enumerate(relation_type_ranking, 1):
        print(f"  {rank}. {r['model']}: {r['avg_relation_types']} ç§ (æ€»å…± {r['unique_relation_types']} ç§ä¸åŒç±»å‹)")
    
    print("\n" + "=" * 80)
    print("âœ… åˆ†æå®Œæˆ!")
    print("=" * 80)
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"   - æ±‡æ€»æŠ¥å‘Š: extraction_summary.md")
    print(f"   - DeepSeekç»Ÿè®¡: deepseek/paper_statistics.csv")
    print(f"   - Geminiç»Ÿè®¡: gemini/paper_statistics.csv")
    print(f"   - Kimiç»Ÿè®¡: kimi/paper_statistics.csv")

if __name__ == "__main__":
    main()
