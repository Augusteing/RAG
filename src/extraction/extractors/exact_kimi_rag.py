# -*- coding: utf-8 -*-
"""
RAG å¢å¼ºç‰ˆ Kimi å®ä½“å…³ç³»æŠ½å–è„šæœ¬
ä½¿ç”¨ LangChain æ£€ç´¢ç›¸å…³æ¨¡å¼ä½œä¸º few-shot ç¤ºä¾‹
é€šè¿‡ Moonshot OpenAI å…¼å®¹æ¥å£è°ƒç”¨ Kimi
"""
import os
import json
import time
import argparse
from datetime import datetime, timezone
from pathlib import Path

# OpenAI SDK (ç”¨äº Kimi/Moonshot OpenAI å…¼å®¹æ¥å£)
from openai import OpenAI

# LangChain RAG ç»„ä»¶
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# è¿›åº¦æ¡
from tqdm import tqdm

# ------------------------------
# è·¯å¾„é…ç½®
# ------------------------------
CODE_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = CODE_DIR.parent.parent.parent

# Schema æ–‡ä»¶
SCHEMA_FILE = PROJECT_ROOT / "configs" / "schemas" / "phm_semantic_patterns.json"

# Prompt æ¨¡æ¿
PROMPT_FILE = PROJECT_ROOT / "configs" / "prompts" / "prompt.txt"  # é€šç”¨/å›é€€æ¨¡æ¿ï¼ˆä¸ä¿®æ”¹ï¼‰
EXP02_PROMPT_FILE = PROJECT_ROOT / "configs" / "prompts" / "exp02" / "prompt.txt"  # ä»…å®éªŒäºŒä½¿ç”¨çš„æ–°è·¯å¾„
EXP03_PROMPTS_DIR = PROJECT_ROOT / "configs" / "prompts" / "exp03"

# è®ºæ–‡ç›®å½•ï¼ˆä½¿ç”¨å¤„ç†åçš„æ¸…æ´—æ•°æ®ï¼‰
PAPERS_DIR = PROJECT_ROOT / "data" / "processed" / "papers"

# å‘é‡æ•°æ®åº“è·¯å¾„
VECTOR_DB_DIR = PROJECT_ROOT / "data" / "vectorstores" / "langchain_chroma_db"

# å®éªŒé€‰æ‹©ï¼šå‘½ä»¤è¡Œä¼˜å…ˆï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡ï¼Œé»˜è®¤ exp02
_parser = argparse.ArgumentParser(add_help=False)
_parser.add_argument("--exp", dest="exp_id", choices=["exp02", "exp03"], default=None)
_parser.add_argument("--exp02", dest="exp02", action="store_true")
_parser.add_argument("--exp03", dest="exp03", action="store_true")
_parser.add_argument("--03", dest="exp03_short", action="store_true")
_parser.add_argument("--max", dest="max_papers", type=int, default=None)
_args, _unknown = _parser.parse_known_args()
_env_exp = os.getenv("EXP_ID")
if _args.exp_id in {"exp02", "exp03"}:
    EXP_ID = _args.exp_id
elif _args.exp03 or _args.exp03_short:
    EXP_ID = "exp03"
elif _args.exp02:
    EXP_ID = "exp02"
elif _env_exp in {"exp02", "exp03"}:
    EXP_ID = _env_exp
else:
    EXP_ID = "exp02"

# é™åˆ¶å¤„ç†ç¯‡æ•°ï¼ˆå¯é€‰ï¼‰
MAX_PAPERS = _args.max_papers
if MAX_PAPERS is None:
    try:
        MAX_PAPERS = int(os.getenv("MAX_PAPERS", "0")) or None
    except Exception:
        MAX_PAPERS = None

# è¾“å‡ºç›®å½•ï¼ˆæŒ‰å®éªŒåˆ†æµï¼‰
OUTPUT_DIR = Path(f"E:/langchain/outputs/extractions/{EXP_ID}/kimi_rag")
LOG_DIR = Path(f"E:/langchain/outputs/logs/{EXP_ID}/kimi")
PROMPT_EXAMPLE_DIR = PROJECT_ROOT / "data" / "prompts" / "kimi_rag_examples"

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(PROMPT_EXAMPLE_DIR, exist_ok=True)

# ------------------------------
# RAG é…ç½®
# ------------------------------
RAG_TOP_K = int(os.getenv("RAG_TOP_K", "5"))
RAG_ENABLED = os.getenv("RAG_ENABLED", "1") == "1"

# Embedding æœ¬åœ°åŒ–é…ç½®
# å¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼š
#   BGE_LOCAL_PATH: æ¨¡å‹æœ¬åœ°ç›®å½•ï¼ˆé»˜è®¤ä¸º E:\langchain\configs\models\bge-large-zh-v1.5ï¼‰
#   EMBEDDING_DEVICE: cuda/cpuï¼ˆé»˜è®¤ cudaï¼‰
#   EMBEDDING_LOCAL_ONLY: 1/0ï¼ˆé»˜è®¤ 1ï¼Œå¼ºåˆ¶ä»…æœ¬åœ°ï¼Œä¸è§¦ç½‘ï¼‰
BGE_LOCAL_PATH = Path(os.getenv("BGE_LOCAL_PATH", r"E:\\langchain\\configs\\models\\bge-large-zh-v1.5"))
EMBEDDING_DEVICE = os.getenv("EMBEDDING_DEVICE", "cuda")
EMBEDDING_LOCAL_ONLY = os.getenv("EMBEDDING_LOCAL_ONLY", "1") == "1"

# Kimi é…ç½® - ä½¿ç”¨ kimi-latest è·å¾—æœ€å¼ºæ€§èƒ½
# kimi-latest æ˜¯æœˆä¹‹æš—é¢æœ€æ–°æœ€å¼ºçš„æ¨¡å‹ï¼ŒåŸºäºä¸‡äº¿å‚æ•°çš„ Kimi K2
# å…·æœ‰æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ã€æ›´å¥½çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›å’Œå®æ—¶æ›´æ–°çš„ç‰¹æ€§
MODEL_NAME = "kimi-latest"
PROVIDER_NAME = "kimi_rag"

print("=" * 80)
print("ğŸš€ RAG å¢å¼ºç‰ˆ Kimi å®ä½“å…³ç³»æŠ½å–")
print("=" * 80)
print(f"\né…ç½®:")
print(f"  - RAG å¯ç”¨: {'æ˜¯' if RAG_ENABLED else 'å¦'}")
print(f"  - RAG Top-K: {RAG_TOP_K}")
print(f"  - æ¨¡å‹: {MODEL_NAME}")
print(f"  - å®éªŒ: {EXP_ID}")
print(f"  - å‘é‡åº“: {VECTOR_DB_DIR.name}")
print(f"  - æœ€å¤§ç¯‡æ•°: {MAX_PAPERS if MAX_PAPERS else 'ä¸é™'}")

# ------------------------------
# åˆå§‹åŒ– LangChain RAG ç³»ç»Ÿ
# ------------------------------
print(f"\n{'='*80}")
print(f"ğŸ“¥ åˆå§‹åŒ– RAG ç³»ç»Ÿ")
print(f"{'='*80}")

if RAG_ENABLED:
    try:
        print(f"â³ åŠ è½½æœ¬åœ° BGE-large-zh-v1.5 Embeddings...")
        print(f"   - æœ¬åœ°æ¨¡å‹ç›®å½•: {BGE_LOCAL_PATH}")
        if not BGE_LOCAL_PATH.exists():
            raise FileNotFoundError(
                f"æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ç›®å½•: {BGE_LOCAL_PATH}. è¯·å°† BAAI/bge-large-zh-v1.5 ä¸‹è½½åˆ°è¯¥ç›®å½•ï¼Œæˆ–è®¾ç½®ç¯å¢ƒå˜é‡ BGE_LOCAL_PATH æŒ‡å‘æœ¬åœ°æ¨¡å‹ç›®å½•ã€‚ å»ºè®®ç›®å½•: E:/langchain/configs/models/bge-large-zh-v1.5"
            )
        
        embeddings = HuggingFaceEmbeddings(
            model_name=str(BGE_LOCAL_PATH),
            model_kwargs={
                'device': EMBEDDING_DEVICE,
                'local_files_only': EMBEDDING_LOCAL_ONLY
            },
            encode_kwargs={'normalize_embeddings': True}
        )
        print(f"âœ… æœ¬åœ° Embeddings åŠ è½½å®Œæˆ ({EMBEDDING_DEVICE}, local_only={EMBEDDING_LOCAL_ONLY})")
        
        print(f"â³ è¿æ¥å‘é‡æ•°æ®åº“...")
        vectorstore = Chroma(
            persist_directory=str(VECTOR_DB_DIR),
            embedding_function=embeddings,
            collection_name="phm_dependency_patterns_langchain"
        )
        
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": RAG_TOP_K}
        )
        
        pattern_count = vectorstore._collection.count()
        print(f"âœ… å‘é‡æ•°æ®åº“è¿æ¥æˆåŠŸ")
        print(f"   - æ¨¡å¼æ€»æ•°: {pattern_count}")
        print(f"   - æ£€ç´¢ç­–ç•¥: Top-{RAG_TOP_K} ç›¸ä¼¼åº¦")
        
    except Exception as e:
        print(f"âŒ RAG åˆå§‹åŒ–å¤±è´¥: {e}")
        # å¦‚æœæ˜¯å› ä¸ºæœ¬åœ°åŒ–æœªå‡†å¤‡å¥½ï¼Œç»™å‡ºæ˜ç¡®æŒ‡å¼•
        print("   æç¤º: ç¡®ä¿å·²å°† BAAI/bge-large-zh-v1.5 æ¨¡å‹ä¸‹è½½è‡³æœ¬åœ°ï¼Œå¹¶è®¾ç½® BGE_LOCAL_PATHï¼Œ"
              "æˆ–åˆ›å»º E:\\langchain\\configs\\models\\bge-large-zh-v1.5 ç›®å½•ã€‚")
        print("   ç°å°†å›é€€åˆ°é RAG æ¨¡å¼ä»¥ç»§ç»­è¿è¡Œï¼ˆä¸è¿›è¡Œæ£€ç´¢å¢å¼ºï¼‰ã€‚")
        RAG_ENABLED = False
else:
    print(f"â„¹ï¸ RAG æœªå¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼")

# ------------------------------
# å·¥å…·å‡½æ•°
# ------------------------------
def now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()

def load_schema(schema_path: Path) -> dict:
    """åŠ è½½ Schema æ–‡ä»¶"""
    with open(schema_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_prompt_template() -> str:
    """åŠ è½½é»˜è®¤ Prompt æ¨¡æ¿æ–‡ä»¶
    - å®éªŒäºŒï¼ˆexp02ï¼‰è¯»å–æ–°è·¯å¾„ï¼šconfigs/prompts/exp02/prompt.txt
    - å…¶ä»–æƒ…å†µä¿æŒå›é€€è‡³é€šç”¨æ¨¡æ¿ï¼ˆä¸æ”¹åŠ¨ï¼‰
    """
    path = EXP02_PROMPT_FILE if EXP_ID == "exp02" else PROMPT_FILE
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()

def load_exp03_prompt_for_paper(paper_file: str) -> str | None:
    """ä¸ºå®éªŒä¸‰åŠ è½½é€è®ºæ–‡ Promptï¼ŒæŒ‰è®ºæ–‡æ–‡ä»¶åæ— æ‰©å±•ååŒ¹é… .txt æˆ– .md"""
    try:
        base = os.path.splitext(os.path.basename(paper_file.replace('priority/', '')))[0]
        candidates = [
            EXP03_PROMPTS_DIR / f"{base}.txt",
            EXP03_PROMPTS_DIR / f"{base}.md",
            EXP03_PROMPTS_DIR / f"{base}_prompt.txt",
            EXP03_PROMPTS_DIR / f"prompt_{base}.txt",
            EXP03_PROMPTS_DIR / f"prompt_{base}.md",
        ]
        for p in candidates:
            if p.exists():
                with open(p, 'r', encoding='utf-8') as f:
                    return f.read()
    except Exception:
        pass
    return None

def extract_abstract(paper_text: str, max_length: int = 500) -> str:
    """æå–è®ºæ–‡æ‘˜è¦ç”¨äºæ£€ç´¢"""
    lines = paper_text.split('\n')
    
    abstract_start = -1
    abstract_end = -1
    
    for i, line in enumerate(lines):
        line_lower = line.strip().lower()
        
        if abstract_start == -1:
            if line_lower.startswith('## æ‘˜è¦') or line_lower.startswith('## abstract'):
                abstract_start = i + 1
                continue
        
        if abstract_start != -1 and abstract_end == -1:
            if line.strip().startswith('## ') and i > abstract_start:
                abstract_end = i
                break
    
    if abstract_start != -1:
        if abstract_end == -1:
            abstract_end = min(abstract_start + 20, len(lines))
        
        abstract_lines = lines[abstract_start:abstract_end]
        abstract_text = '\n'.join(abstract_lines).strip()
        
        if len(abstract_text) > 50:
            return abstract_text[:max_length]
    
    text_start = 0
    for i, line in enumerate(lines):
        if line.strip() and not line.strip().startswith('#'):
            text_start = i
            break
    
    content = '\n'.join(lines[text_start:])
    return content[:max_length]

def retrieve_relevant_patterns(query_text: str, top_k: int = RAG_TOP_K) -> list:
    """ä½¿ç”¨ LangChain æ£€ç´¢ç›¸å…³æ¨¡å¼"""
    if not RAG_ENABLED:
        return []
    
    try:
        abstract = extract_abstract(query_text)
        relevant_docs = retriever.get_relevant_documents(abstract)
        
        patterns = []
        for doc in relevant_docs:
            pattern_info = {
                'semantic_pattern': doc.metadata.get('semantic_pattern', 'N/A'),
                'frequency': doc.metadata.get('frequency', 0),
                'syntactic_path': doc.metadata.get('syntactic_path', 'N/A'),
                'content': doc.page_content
            }
            patterns.append(pattern_info)
        
        return patterns
    except Exception as e:
        print(f"âš ï¸ æ£€ç´¢å¤±è´¥: {e}")
        return []

def translate_entity_to_chinese(entity: str) -> str:
    """å°†è‹±æ–‡å®ä½“ç±»å‹ç¿»è¯‘ä¸ºä¸­æ–‡"""
    translation_map = {
        'Performance Metric': 'æ€§èƒ½æŒ‡æ ‡',
        'Model': 'æ¨¡å‹',
        'model': 'æ¨¡å‹',
        'Dataset': 'æ•°æ®é›†',
        'Problem': 'é—®é¢˜',
        'Method': 'æ–¹æ³•',
        'System': 'ç³»ç»Ÿ',
        'Technology': 'æŠ€æœ¯',
        'Algorithm': 'ç®—æ³•',
        'Data': 'æ•°æ®',
        'Task': 'ä»»åŠ¡',
        'Feature': 'ç‰¹å¾',
        'Parameter': 'å‚æ•°',
        'Indicator': 'æŒ‡æ ‡',
        'Application': 'åº”ç”¨',
        'Attribute': 'å±æ€§',
        'Component': 'éƒ¨ä»¶',
        'Fault': 'æ•…éšœ',
        'Equipment': 'è£…å¤‡',
        'Tool': 'å·¥å…·',
        'Standard': 'æ ‡å‡†',
        'Process': 'æµç¨‹'
    }
    
    for en, zh in translation_map.items():
        entity = entity.replace(en, zh)
    
    return entity

def format_patterns_for_prompt(patterns: list) -> str:
    """å°†æ£€ç´¢åˆ°çš„æ¨¡å¼æ ¼å¼åŒ–ä¸º Prompt æ–‡æœ¬ï¼ˆä¸­æ–‡ç‰ˆï¼Œæ— é¢‘æ¬¡ï¼‰"""
    if not patterns:
        return ""
    
    lines = ["\n  **æœ¬è®ºæ–‡ç›¸å…³çš„ä¾å­˜è·¯å¾„æ¨¡å¼ï¼ˆä»1,481æ¡ä¸­åŠ¨æ€æ£€ç´¢ï¼‰**\n"]
    lines.append(f"åŸºäºè®ºæ–‡æ‘˜è¦è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œæ£€ç´¢åˆ°ä»¥ä¸‹ {len(patterns)} ä¸ªæœ€ç›¸å…³çš„å®ä½“å…³ç³»æ¨¡å¼ä½œä¸ºå‚è€ƒï¼š\n")
    
    for i, pattern in enumerate(patterns, 1):
        semantic = pattern['semantic_pattern']
        semantic_cn = translate_entity_to_chinese(semantic)
        lines.append(f"{i}. **{semantic_cn}**")
    
    lines.append("\nğŸ’¡ **è¯·å‚è€ƒä»¥ä¸Šæ¨¡å¼è¿›è¡Œå®ä½“å…³ç³»æŠ½å–**ï¼Œè¿™äº›æ¨¡å¼ä»£è¡¨äº†PHMé¢†åŸŸå¸¸è§çš„è¯­ä¹‰å…³ç³»ã€‚\n")
    
    return "\n".join(lines)

def build_enhanced_prompt(paper_text: str, schema: dict, *, paper_file: str | None = None) -> tuple:
    """æ„å»º RAG å¢å¼ºçš„ Prompt"""
    # æ ¹æ®å®éªŒé€‰æ‹©æ¨¡æ¿ï¼šexp03 ä¼˜å…ˆé€è®ºæ–‡æ¨¡æ¿ï¼Œå¦åˆ™é»˜è®¤æ¨¡æ¿
    prompt_template = None
    if EXP_ID == "exp03" and paper_file:
        prompt_template = load_exp03_prompt_for_paper(paper_file)
        if prompt_template:
            print("   ğŸ§© ä½¿ç”¨ exp03 é€è®ºæ–‡ Prompt æ¨¡æ¿")
        else:
            print("   âš ï¸ æœªæ‰¾åˆ°å¯¹åº” exp03 Promptï¼Œå›é€€åˆ°é€šç”¨æ¨¡æ¿")
    if not prompt_template:
        prompt_template = load_prompt_template()
    
    retrieved_patterns = []
    rag_section = ""
    
    if RAG_ENABLED:
        print(f"   ğŸ” æ£€ç´¢ç›¸å…³æ¨¡å¼...")
        retrieved_patterns = retrieve_relevant_patterns(paper_text)
        if retrieved_patterns:
            rag_section = format_patterns_for_prompt(retrieved_patterns)
            print(f"   âœ… æ£€ç´¢åˆ° {len(retrieved_patterns)} ä¸ªç›¸å…³æ¨¡å¼")
        else:
            print(f"   âš ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³æ¨¡å¼")
    
    enhanced_prompt = prompt_template.replace(
        '{schema_placeholder}', 
        rag_section
    ).replace(
        '{full_text_placeholder}',
        paper_text
    )
    
    return enhanced_prompt, retrieved_patterns

def strip_code_fences(s: str) -> str:
    """å»é™¤ ```json ... ``` æ ·å¼çš„å›´æ ï¼Œå¹¶ç§»é™¤è¯­è¨€è¡Œ"""
    if not isinstance(s, str):
        return s
    s = s.strip()
    
    # å¤„ç† markdown ä»£ç å—
    if s.startswith("```") and s.endswith("```"):
        s = s[3:-3].strip()
        # å¯èƒ½ä»¥è¯­è¨€æ ‡è®°å¼€å¤´ï¼Œæ¯”å¦‚ "json"
        if "\n" in s:
            first_line, rest = s.split("\n", 1)
            if first_line.strip().lower() in {"json", "js", "javascript"}:
                s = rest
    
    # å¤„ç†é¢å¤–è¯´æ˜æ–‡æœ¬
    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }
    first_brace = s.find('{')
    last_brace = s.rfind('}')
    
    if first_brace != -1 and last_brace != -1 and first_brace < last_brace:
        s = s[first_brace:last_brace + 1]
    
    return s

def parse_json_response(content: str) -> dict:
    """å°†æ¨¡å‹è¾“å‡ºè§£æä¸ºä¸¥æ ¼ JSONï¼Œè‡ªåŠ¨å‰¥ç¦»ä»£ç å›´æ å’Œé¢å¤–æ–‡æœ¬"""
    if not content:
        raise ValueError("API è¿”å›äº†ç©ºå†…å®¹")
    
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        cleaned = strip_code_fences(content)
        try:
            return json.loads(cleaned)
        except json.JSONDecodeError as e:
            # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
            print(f"\nâŒ JSON è§£æå¤±è´¥:")
            print(f"   é”™è¯¯: {e}")
            print(f"   åŸå§‹å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"   æ¸…ç†åå†…å®¹é•¿åº¦: {len(cleaned)} å­—ç¬¦")
            print(f"   åŸå§‹å†…å®¹é¢„è§ˆ (å‰500å­—ç¬¦):")
            print(f"   {content[:500]}")
            print(f"   æ¸…ç†åå†…å®¹é¢„è§ˆ:")
            print(f"   {cleaned[:500]}")
            raise ValueError(f"æ— æ³•è§£æ JSON: {e}ã€‚æ¸…ç†åå†…å®¹: {cleaned[:200]}...")

# ------------------------------
# åˆå§‹åŒ– Kimi å®¢æˆ·ç«¯ (Moonshot API)
# ------------------------------
api_key = os.getenv("KIMI_API_KEY")
if not api_key:
    raise ValueError("è¯·è®¾ç½® KIMI_API_KEY ç¯å¢ƒå˜é‡")

client = OpenAI(
    api_key=api_key, 
    base_url="https://api.moonshot.cn/v1"  # Moonshot/Kimi OpenAI å…¼å®¹ç«¯ç‚¹
)

# ------------------------------
# åŠ è½½ Schema
# ------------------------------
print(f"\n{'='*80}")
print(f"ğŸ“‚ åŠ è½½é…ç½®æ–‡ä»¶")
print(f"{'='*80}")

schema_data = load_schema(SCHEMA_FILE)
print(f"âœ… Schema åŠ è½½å®Œæˆ")
print(f"   - å®ä½“ç±»å‹: {len(schema_data.get('entity_types', {}))} ç§")
print(f"   - å…³ç³»ç±»å‹: {len(schema_data.get('relation_types', {}))} ç§")

# ------------------------------
# è·å–è®ºæ–‡åˆ—è¡¨ï¼ˆæŒ‰é˜¶æ®µï¼štest â†’ priority_remaining â†’ othersï¼‰
# ------------------------------
test_dir = PAPERS_DIR / "test"
priority_remaining_dir = PAPERS_DIR / "priority_remaining"
others_dir = PAPERS_DIR / "others"

def _collect_md_files(dir_path: Path, prefix: str) -> list:
    if dir_path.exists():
        return sorted([f"{prefix}/{f}" for f in os.listdir(dir_path) if f.endswith('.md')])
    return []

test_papers = _collect_md_files(test_dir, "test")
priority_remaining_papers = _collect_md_files(priority_remaining_dir, "priority_remaining")
others_papers = _collect_md_files(others_dir, "others")

# åˆå¹¶å¹¶æŒ‰æœ€å¤§ç¯‡æ•°è£å‰ªï¼ˆè·¨é˜¶æ®µæ•´ä½“è£å‰ªï¼‰
_all_papers = test_papers + priority_remaining_papers + others_papers
_total_before = len(_all_papers)
if MAX_PAPERS and MAX_PAPERS > 0:
    _allowed = set(_all_papers[:MAX_PAPERS])
    test_papers = [p for p in test_papers if p in _allowed]
    priority_remaining_papers = [p for p in priority_remaining_papers if p in _allowed]
    others_papers = [p for p in others_papers if p in _allowed]
    print(f"ğŸ”¢ ä»…å¤„ç†å‰ {len(_allowed)} ç¯‡ï¼ˆåŸ {_total_before} ç¯‡ï¼‰")

print(f"\nâœ… æ‰¾åˆ° {len(test_papers) + len(priority_remaining_papers) + len(others_papers)} ç¯‡è®ºæ–‡")
print(f"   - Test è®ºæ–‡: {len(test_papers)} ç¯‡")
print(f"   - Priority Remaining è®ºæ–‡: {len(priority_remaining_papers)} ç¯‡")
print(f"   - Others è®ºæ–‡: {len(others_papers)} ç¯‡")

stages = [
    ("test", test_papers),
    ("priority_remaining", priority_remaining_papers),
    ("others", others_papers),
]

# ------------------------------
# ä¸»å¤„ç†å¾ªç¯
# ------------------------------
print(f"\n{'='*80}")
print(f"ğŸ”„ å¼€å§‹æŠ½å–")
print(f"{'='*80}")

success_count = 0
failed_count = 0
extraction_logs = []

_total_to_process = sum(len(s[1]) for s in stages)
_processed_so_far = 0

for _stage_idx, (stage_name, stage_papers) in enumerate(stages):
    if not stage_papers:
        continue
    tqdm.write(f"\né˜¶æ®µï¼š{stage_name}ï¼ˆ{len(stage_papers)} ç¯‡ï¼‰")
    for j, paper_file in enumerate(tqdm(stage_papers, desc=f"{stage_name} é˜¶æ®µ", unit="ç¯‡"), 1):
        i = _processed_so_far + j
        tqdm.write(f"\n[{i}/{_total_to_process}] å¤„ç†: {paper_file}")
        
        # è¾“å‡ºæŒ‰é˜¶æ®µåˆ†ç›®å½•ï¼štest/ã€priority_remain/ã€others/
        _stage_dir = "priority_remain" if stage_name == "priority_remaining" else stage_name
        output_file = OUTPUT_DIR / _stage_dir / os.path.basename(paper_file).replace('.md', '.json')
        os.makedirs(output_file.parent, exist_ok=True)
        if output_file.exists():
            tqdm.write(f"   â­ï¸ å·²å­˜åœ¨ï¼Œè·³è¿‡")
            continue
        
        try:
            paper_start_time = time.time()
            
            # 1. è¯»å–è®ºæ–‡
            if paper_file.startswith('test/'):
                paper_path = PAPERS_DIR / 'test' / os.path.basename(paper_file)
            elif paper_file.startswith('priority_remaining/'):
                paper_path = PAPERS_DIR / 'priority_remaining' / os.path.basename(paper_file)
            elif paper_file.startswith('others/'):
                paper_path = PAPERS_DIR / 'others' / os.path.basename(paper_file)
            else:
                paper_path = PAPERS_DIR / paper_file
            
            with open(paper_path, 'r', encoding='utf-8') as f:
                paper_text = f.read()
            
            tqdm.write(f"   ğŸ“„ è®ºæ–‡é•¿åº¦: {len(paper_text)} å­—ç¬¦")
            
            # 2. æ„å»º RAG å¢å¼º Prompt
            start_time = time.time()
            enhanced_prompt, retrieved_patterns = build_enhanced_prompt(paper_text, schema_data, paper_file=paper_file)
            prompt_time = time.time() - start_time
            
            tqdm.write(f"   âœ… Prompt æ„å»ºå®Œæˆ ({prompt_time:.2f}s)")
            tqdm.write(f"   ğŸ“Š Prompt é•¿åº¦: {len(enhanced_prompt)} å­—ç¬¦")
            
            # ç¤ºä¾‹ Prompt ä¸å†ä¿å­˜ä¸º txtï¼ˆä»…ä¿å­˜ JSON ç»“æœï¼‰
            
            # 3. è°ƒç”¨ Kimi API
            tqdm.write(f"   â³ è°ƒç”¨ Kimi API...")
            api_start = time.time()
            
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ PHM é¢†åŸŸçš„çŸ¥è¯†æŠ½å–ä¸“å®¶ã€‚åªè¾“å‡ºä¸¥æ ¼çš„ JSONï¼Œä¸æ·»åŠ ä»»ä½•è§£é‡Šã€‚"},
                    {"role": "user", "content": enhanced_prompt}
                ],
                temperature=0,
                max_tokens=4096,
                response_format={"type": "json_object"}
            )
            
            api_time = time.time() - api_start
            content = response.choices[0].message.content
            
            tqdm.write(f"   âœ… API è°ƒç”¨å®Œæˆ ({api_time:.2f}s)")
            
            # 4. è§£æç»“æœ
            result = parse_json_response(content)
            
            # 5. ä¿å­˜ç»“æœï¼ˆå¹²å‡€æ ¼å¼ï¼‰
            output_data = {
                "entities": result.get("entities", []),
                "relations": result.get("relations", [])
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            paper_total_time = time.time() - paper_start_time
            
            tqdm.write(f"   âœ… ç»“æœå·²ä¿å­˜: {output_file.name}")
            tqdm.write(f"   ğŸ“Š æŠ½å–ç»Ÿè®¡: {len(result.get('entities', []))} å®ä½“, {len(result.get('relations', []))} å…³ç³»")
            tqdm.write(f"   â±ï¸ æ€»è€—æ—¶: {paper_total_time:.2f}s")
            
            # è®°å½•æ—¥å¿—
            extraction_logs.append({
                "paper": paper_file,
                "timestamp": now_iso(),
                "duration_seconds": round(paper_total_time, 3),
                "entity_count": len(result.get('entities', [])),
                "relation_count": len(result.get('relations', [])),
                "rag_enabled": RAG_ENABLED,
                "rag_top_k": RAG_TOP_K if RAG_ENABLED else 0,
                "retrieved_patterns": [p['semantic_pattern'] for p in retrieved_patterns],
                "prompt_tokens": getattr(response.usage, 'prompt_tokens', 0),
                "completion_tokens": getattr(response.usage, 'completion_tokens', 0),
                "total_tokens": getattr(response.usage, 'total_tokens', 0),
                "success": True
            })
            
            success_count += 1
            
        except Exception as e:
            tqdm.write(f"   âŒ å¤„ç†å¤±è´¥: {e}")
            extraction_logs.append({
                "paper": paper_file,
                "timestamp": now_iso(),
                "success": False,
                "error": str(e)
            })
            failed_count += 1
        
        # ä¸å†åœ¨ç¬¬ 10 ç¯‡åè¯¢é—®æ˜¯å¦ç»§ç»­

    _processed_so_far += len(stage_papers)

    # é˜¶æ®µç»“æŸåè¯¢é—®æ˜¯å¦ç»§ç»­ä¸‹ä¸€é˜¶æ®µ
    if _stage_idx < len(stages) - 1:
        tqdm.write(f"\n{'='*80}")
        tqdm.write(f"â¸ï¸  å·²å®Œæˆ {stage_name} é˜¶æ®µæŠ½å–")
        tqdm.write(f"{'='*80}")
        _next_stage_name = stages[_stage_idx + 1][0]
        _ans = input(f"æ˜¯å¦ç»§ç»­å¤„ç†ä¸‹ä¸€é˜¶æ®µï¼ˆ{_next_stage_name}ï¼‰ï¼Ÿ(y/n): ").strip().lower()
        if _ans != 'y':
            tqdm.write(f"\nğŸ›‘ ç”¨æˆ·é€‰æ‹©åœæ­¢")
            break
        tqdm.write(f"\nâ–¶ï¸  ç»§ç»­å¤„ç†...\n")
    
    time.sleep(1)

# ------------------------------
# æ€»ç»“
# ------------------------------
print(f"\n{'='*80}")
print(f"âœ… æŠ½å–å®Œæˆ")
print(f"{'='*80}")
print(f"ğŸ“Š ç»Ÿè®¡:")
print(f"   - æˆåŠŸ: {success_count}")
print(f"   - å¤±è´¥: {failed_count}")
print(f"   - æ€»è®¡: {len(papers)}")
print(f"\nğŸ’¾ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")

# ä¿å­˜æ—¥å¿—
if extraction_logs:
    log_file = LOG_DIR / f"extraction_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump({
            "total_papers": len(papers),
            "successful_extractions": success_count,
            "failed_extractions": failed_count,
            "logs": extraction_logs
        }, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ æ—¥å¿—å·²ä¿å­˜: {log_file}")

print(f"{'='*80}")
