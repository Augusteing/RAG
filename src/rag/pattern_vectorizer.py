# -*- coding: utf-8 -*-
"""
æ¨¡å¼å‘é‡åŒ–æ¨¡å—ï¼šå°†ä¾å­˜å¥æ³•åˆ†æå¾—åˆ°çš„æ¨¡å¼åŠ è½½å¹¶å‘é‡åŒ–å­˜å‚¨
"""
import os
import csv
import json
from typing import List, Dict, Any

try:
    from langchain_openai import OpenAIEmbeddings
    from langchain_community.vectorstores import Chroma
    from langchain_core.documents import Document
except ImportError:
    print("âš ï¸  è­¦å‘Šï¼šLangChainç›¸å…³åŒ…æœªå®‰è£…ï¼Œè¯·å…ˆè¿è¡Œ: pip install -r requirements.txt")
    raise


class PatternVectorizer:
    """å°†æ¨¡å¼æ•°æ®å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“"""
    
    def __init__(
        self,
        csv_path: str,
        json_path: str,
        persist_directory: str = "./chroma_db",
        collection_name: str = "phm_patterns",
        embedding_model: str = "text-embedding-3-small"
    ):
        """
        åˆå§‹åŒ–å‘é‡åŒ–å™¨
        
        Args:
            csv_path: CSVæ¨¡å¼æ–‡ä»¶è·¯å¾„
            json_path: JSON schemaæ–‡ä»¶è·¯å¾„
            persist_directory: ChromaDBæŒä¹…åŒ–ç›®å½•
            collection_name: é›†åˆåç§°
            embedding_model: OpenAI embeddingæ¨¡å‹åç§°
        """
        self.csv_path = csv_path
        self.json_path = json_path
        self.persist_directory = persist_directory
        self.collection_name = collection_name
        
        # åˆå§‹åŒ– embedding æ¨¡å‹
        self.embeddings = OpenAIEmbeddings(
            model=embedding_model,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.vectorstore = None
        
    def load_csv_patterns(self) -> List[Dict[str, Any]]:
        """ä»CSVæ–‡ä»¶åŠ è½½æ¨¡å¼æ•°æ®"""
        patterns = []
        
        with open(self.csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # è§£æå¥æ³•è·¯å¾„å­—ç¬¦ä¸²
                syntactic_paths = self._parse_syntactic_paths(
                    row['å¥æ³•å®ç°è·¯å¾„ (Syntactic Realizations)']
                )
                
                pattern = {
                    'semantic_pattern': row['è¯­ä¹‰æ¨¡å¼ (Semantic Pattern)'],
                    'total_frequency': int(row['æ€»é¢‘æ¬¡ (Total Freq)']),
                    'syntactic_paths': syntactic_paths
                }
                patterns.append(pattern)
        
        print(f"âœ… ä»CSVåŠ è½½äº† {len(patterns)} æ¡æ¨¡å¼")
        return patterns
    
    def _parse_syntactic_paths(self, path_str: str) -> List[Dict[str, Any]]:
        """è§£æå¥æ³•è·¯å¾„å­—ç¬¦ä¸²"""
        paths = []
        
        # ç®€å•è§£æï¼ŒæŒ‰ç…§ "- å¥æ³•è·¯å¾„:" åˆ†å‰²
        parts = path_str.split('- å¥æ³•è·¯å¾„:')
        
        for part in parts[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªç©ºå…ƒç´ 
            if not part.strip():
                continue
                
            # æå–é¢‘æ¬¡å’Œæ ·ä¾‹
            lines = part.strip().split('\n')
            if not lines:
                continue
            
            path_line = lines[0].strip()
            
            # æå–é¢‘æ¬¡ (é¢‘æ¬¡: X, æ ·ä¾‹: ...)
            frequency = 1
            example_head = ""
            example_tail = ""
            
            if 'é¢‘æ¬¡:' in path_line:
                try:
                    freq_part = path_line.split('é¢‘æ¬¡:')[1].split(',')[0].strip()
                    frequency = int(freq_part)
                except:
                    pass
            
            if 'æ ·ä¾‹:' in path_line:
                try:
                    example_part = path_line.split('æ ·ä¾‹:')[1].strip()
                    # æ ¼å¼: [å¤´å®ä½“] â†’ [å°¾å®ä½“])
                    if '[' in example_part and ']' in example_part:
                        parts = example_part.split('â†’')
                        if len(parts) == 2:
                            example_head = parts[0].strip().strip('[]')
                            example_tail = parts[1].strip().strip('[]').rstrip(')')
                except:
                    pass
            
            paths.append({
                'path': path_line,
                'frequency': frequency,
                'example_head': example_head,
                'example_tail': example_tail
            })
        
        return paths
    
    def load_json_schema(self) -> Dict[str, Any]:
        """ä»JSONæ–‡ä»¶åŠ è½½schemaå®šä¹‰"""
        with open(self.json_path, 'r', encoding='utf-8') as f:
            schema = json.load(f)
        
        print(f"âœ… åŠ è½½äº†JSON schemaï¼ŒåŒ…å« {len(schema.get('entity_types', {}))} ç§å®ä½“ç±»å‹")
        return schema
    
    def create_documents(
        self,
        csv_patterns: List[Dict[str, Any]],
        json_schema: Dict[str, Any]
    ) -> List[Document]:
        """åˆ›å»ºLangChain Documentå¯¹è±¡"""
        documents = []
        
        # 1. ä»CSVæ¨¡å¼åˆ›å»ºæ–‡æ¡£
        for pattern in csv_patterns:
            semantic_pattern = pattern['semantic_pattern']
            total_freq = pattern['total_frequency']
            
            # ä¸ºæ¯ä¸ªå¥æ³•è·¯å¾„åˆ›å»ºä¸€ä¸ªæ–‡æ¡£
            for syn_path in pattern['syntactic_paths']:
                # æ„é€ æ–‡æ¡£å†…å®¹ï¼ˆç”¨äºå‘é‡åŒ–çš„æ–‡æœ¬ï¼‰
                content = f"""è¯­ä¹‰æ¨¡å¼: {semantic_pattern}
ç¤ºä¾‹: {syn_path['example_head']} â†’ {syn_path['example_tail']}
é¢‘æ¬¡: {syn_path['frequency']}
å¥æ³•è·¯å¾„: {syn_path['path'][:200]}"""  # æˆªæ–­è¿‡é•¿è·¯å¾„
                
                # å…ƒæ•°æ®ï¼ˆä¸å‚ä¸å‘é‡åŒ–ï¼Œä½†æ£€ç´¢æ—¶è¿”å›ï¼‰
                metadata = {
                    'type': 'csv_pattern',
                    'semantic_pattern': semantic_pattern,
                    'total_frequency': total_freq,
                    'example_head': syn_path['example_head'],
                    'example_tail': syn_path['example_tail'],
                    'frequency': syn_path['frequency'],
                    'syntactic_path': syn_path['path'][:500]  # é™åˆ¶é•¿åº¦
                }
                
                documents.append(Document(
                    page_content=content,
                    metadata=metadata
                ))
        
        print(f"âœ… ä»CSVåˆ›å»ºäº† {len(documents)} ä¸ªæ–‡æ¡£")
        
        # 2. ä»JSONçš„high_confidence_patternsåˆ›å»ºæ–‡æ¡£
        if 'high_confidence_patterns' in json_schema:
            for pattern in json_schema['high_confidence_patterns']:
                for example in pattern.get('examples', []):
                    content = f"""è¯­ä¹‰æ¨¡å¼: {pattern['head_type']} â†’ {pattern['relation']} â†’ {pattern['tail_type']}
ç¤ºä¾‹: {example['head']} â†’ {example['tail']}
ç½®ä¿¡åº¦: {pattern['confidence_score']}
è¯´æ˜: {pattern['description']}"""
                    
                    metadata = {
                        'type': 'json_pattern',
                        'pattern_id': pattern['id'],
                        'semantic_template': pattern['semantic_template'],
                        'head_type': pattern['head_type'],
                        'relation': pattern['relation'],
                        'tail_type': pattern['tail_type'],
                        'confidence_score': pattern['confidence_score'],
                        'example_head': example['head'],
                        'example_tail': example['tail'],
                        'description': pattern['description']
                    }
                    
                    documents.append(Document(
                        page_content=content,
                        metadata=metadata
                    ))
        
        print(f"âœ… ä»JSONåˆ›å»ºäº† {len(documents) - len(csv_patterns)} ä¸ªé«˜ç½®ä¿¡åº¦æ¨¡å¼æ–‡æ¡£")
        print(f"ğŸ“Š æ€»è®¡åˆ›å»º {len(documents)} ä¸ªæ–‡æ¡£ç”¨äºå‘é‡åŒ–")
        
        return documents
    
    def build_vectorstore(self, documents: List[Document]) -> Chroma:
        """æ„å»ºå‘é‡æ•°æ®åº“"""
        print("ğŸ”„ å¼€å§‹å‘é‡åŒ–å’Œæ„å»ºæ•°æ®åº“...")
        
        # åˆ›å»ºChromaDBå‘é‡å­˜å‚¨
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            collection_name=self.collection_name,
            persist_directory=self.persist_directory
        )
        
        print(f"âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼Œå·²æŒä¹…åŒ–åˆ°: {self.persist_directory}")
        
        self.vectorstore = vectorstore
        return vectorstore
    
    def run(self) -> Chroma:
        """æ‰§è¡Œå®Œæ•´çš„å‘é‡åŒ–æµç¨‹"""
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹æ„å»ºPHMæ¨¡å¼å‘é‡æ•°æ®åº“")
        print("="*60 + "\n")
        
        # 1. åŠ è½½æ•°æ®
        csv_patterns = self.load_csv_patterns()
        json_schema = self.load_json_schema()
        
        # 2. åˆ›å»ºæ–‡æ¡£
        documents = self.create_documents(csv_patterns, json_schema)
        
        # 3. æ„å»ºå‘é‡åº“
        vectorstore = self.build_vectorstore(documents)
        
        print("\n" + "="*60)
        print("âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
        print("="*60 + "\n")
        
        return vectorstore


def main():
    """ä¸»å‡½æ•°ï¼šæ„å»ºå‘é‡æ•°æ®åº“"""
    # è·¯å¾„é…ç½®
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
    ROOT_DIR = os.path.dirname(os.path.dirname(SCRIPT_DIR))
    
    CSV_PATH = os.path.join(ROOT_DIR, "ä¾å­˜è·¯å¾„æå–ç»“æœ", 
                            "semantic_syntactic_patterns_report_2025-10-14_172830.csv")
    JSON_PATH = os.path.join(ROOT_DIR, "schemaæ–‡ä»¶", "phm_semantic_patterns.json")
    PERSIST_DIR = os.path.join(ROOT_DIR, "chroma_db")
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(CSV_PATH):
        raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {CSV_PATH}")
    if not os.path.exists(JSON_PATH):
        raise FileNotFoundError(f"JSONæ–‡ä»¶ä¸å­˜åœ¨: {JSON_PATH}")
    
    # æ£€æŸ¥API Key
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY")
    
    # åˆ›å»ºå‘é‡åŒ–å™¨å¹¶è¿è¡Œ
    vectorizer = PatternVectorizer(
        csv_path=CSV_PATH,
        json_path=JSON_PATH,
        persist_directory=PERSIST_DIR,
        collection_name="phm_patterns"
    )
    
    vectorstore = vectorizer.run()
    
    # æµ‹è¯•æ£€ç´¢
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•æ£€ç´¢åŠŸèƒ½")
    print("="*60 + "\n")
    
    test_query = "èˆªç©ºå‘åŠ¨æœºå‰©ä½™å¯¿å‘½é¢„æµ‹æ–¹æ³•"
    results = vectorstore.similarity_search(test_query, k=3)
    
    print(f"æŸ¥è¯¢: {test_query}")
    print(f"\næ£€ç´¢åˆ° {len(results)} ä¸ªç›¸å…³æ¨¡å¼:\n")
    
    for i, doc in enumerate(results, 1):
        print(f"ã€ç»“æœ {i}ã€‘")
        print(f"å†…å®¹: {doc.page_content[:200]}...")
        print(f"å…ƒæ•°æ®: {doc.metadata.get('semantic_pattern', 'N/A')}")
        print(f"ç¤ºä¾‹: {doc.metadata.get('example_head', '')} â†’ {doc.metadata.get('example_tail', '')}")
        print("-" * 60)


if __name__ == "__main__":
    main()
