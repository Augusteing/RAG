# -*- coding: utf-8 -*-
"""
åŸºäº LangChain çš„ PHM ä¾å­˜è·¯å¾„æ¨¡å¼å‘é‡æ•°æ®åº“æ„å»º
ä½¿ç”¨ LangChain æ ‡å‡†ç»„ä»¶ï¼šHuggingFaceEmbeddings + Chroma + Retriever
"""
import os
import sys
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# LangChain æ ¸å¿ƒç»„ä»¶
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

print("=" * 80)
print("ğŸ”§ PHM ä¾å­˜è·¯å¾„æ¨¡å¼å‘é‡æ•°æ®åº“ (LangChain ç‰ˆæœ¬)")
print("=" * 80)

# ==================== é…ç½®éƒ¨åˆ† ====================

# è·¯å¾„é…ç½®
CSV_FILE = PROJECT_ROOT / "data" / "processed" / "dependency_patterns" / "semantic_syntactic_patterns_report_2025-10-14_172830.csv"
CHROMA_PERSIST_DIR = str(PROJECT_ROOT / "data" / "vectorstores" / "langchain_chroma_db")
COLLECTION_NAME = "phm_dependency_patterns_langchain"

# LangChain Embedding é…ç½®
EMBEDDING_MODEL = "BAAI/bge-large-zh-v1.5"
EMBEDDING_DEVICE = "cuda"  # æˆ– "cpu"

# æµ‹è¯•æ¨¡å¼
TEST_MODE = os.getenv("TEST_MODE", "0") == "1"
TEST_SAMPLE_SIZE = 20

print(f"\nğŸ“‚ é…ç½®:")
print(f"   CSV æ–‡ä»¶: {CSV_FILE.name}")
print(f"   å‘é‡åº“è·¯å¾„: {CHROMA_PERSIST_DIR}")
print(f"   Embedding æ¨¡å‹: {EMBEDDING_MODEL}")
print(f"   è¿è¡Œè®¾å¤‡: {EMBEDDING_DEVICE}")
print(f"   æµ‹è¯•æ¨¡å¼: {'æ˜¯' if TEST_MODE else 'å¦'}")

# ==================== Step 1: åˆå§‹åŒ– LangChain Embeddings ====================

print("\n" + "=" * 80)
print("Step 1: åˆå§‹åŒ– LangChain HuggingFaceEmbeddings")
print("=" * 80)

# åˆ›å»º LangChain çš„ HuggingFaceEmbeddings
# è¿™ä¼šè‡ªåŠ¨ä½¿ç”¨ sentence-transformers åº“
embeddings = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL,
    model_kwargs={'device': EMBEDDING_DEVICE},
    encode_kwargs={
        'normalize_embeddings': True,  # å½’ä¸€åŒ–å‘é‡ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦å‹å¥½ï¼‰
        'batch_size': 32,              # æ‰¹å¤„ç†å¤§å°
    }
)

print(f"âœ… LangChain Embeddings åˆå§‹åŒ–å®Œæˆ")
print(f"   ç±»å‹: {type(embeddings).__name__}")
print(f"   åº•å±‚æ¨¡å‹: {EMBEDDING_MODEL}")

# æµ‹è¯• embedding
test_text = "æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹è®¾å¤‡æ•…éšœ"
test_embedding = embeddings.embed_query(test_text)
print(f"\nğŸ§ª æµ‹è¯• Embedding:")
print(f"   æ–‡æœ¬: {test_text}")
print(f"   å‘é‡ç»´åº¦: {len(test_embedding)}")
print(f"   å‰5ç»´: {test_embedding[:5]}")

# ==================== Step 2: åŠ è½½å’Œå‡†å¤‡æ•°æ® ====================

print("\n" + "=" * 80)
print("Step 2: åŠ è½½ä¾å­˜è·¯å¾„æ¨¡å¼æ•°æ®")
print("=" * 80)

def load_patterns_as_langchain_documents(csv_path: Path, test_mode: bool = False) -> List[Document]:
    """åŠ è½½ CSV æ•°æ®å¹¶è½¬æ¢ä¸º LangChain Document æ ¼å¼"""
    
    # è¯»å– CSV
    df = pd.read_csv(csv_path)
    
    # æ ‡å‡†åŒ–åˆ—å
    column_mapping = {}
    for col in df.columns:
        if "è¯­ä¹‰æ¨¡å¼" in col:
            column_mapping[col] = "è¯­ä¹‰æ¨¡å¼"
        elif "æ€»é¢‘æ¬¡" in col or "é¢‘æ¬¡" in col:
            column_mapping[col] = "æ€»é¢‘æ¬¡"
        elif "å¥æ³•" in col and "è·¯å¾„" in col:
            column_mapping[col] = "å¥æ³•å®ç°è·¯å¾„"
    
    df = df.rename(columns=column_mapping)
    
    print(f"âœ… åŠ è½½æˆåŠŸ: {len(df)} æ¡æ¨¡å¼")
    
    if test_mode:
        df = df.head(TEST_SAMPLE_SIZE)
        print(f"âš ï¸ æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†å‰ {TEST_SAMPLE_SIZE} æ¡")
    
    # è½¬æ¢ä¸º LangChain Document
    documents = []
    for idx, row in df.iterrows():
        # æ–‡æ¡£å†…å®¹ï¼ˆç”¨äºå‘é‡åŒ–å’Œæ£€ç´¢ï¼‰
        page_content = f"{row['è¯­ä¹‰æ¨¡å¼']} | å¥æ³•è·¯å¾„: {row['å¥æ³•å®ç°è·¯å¾„']}"
        
        # å…ƒæ•°æ®ï¼ˆç”¨äºè¿‡æ»¤å’Œå±•ç¤ºï¼‰
        metadata = {
            "pattern_id": str(idx),
            "semantic_pattern": row['è¯­ä¹‰æ¨¡å¼'],
            "syntactic_path": row['å¥æ³•å®ç°è·¯å¾„'],
            "frequency": int(row['æ€»é¢‘æ¬¡']),
            "source": "dependency_parsing",
            "created_at": datetime.now().isoformat()
        }
        
        documents.append(Document(
            page_content=page_content,
            metadata=metadata
        ))
    
    print(f"âœ… è½¬æ¢å®Œæˆ: {len(documents)} ä¸ª LangChain Documents")
    return documents

documents = load_patterns_as_langchain_documents(CSV_FILE, test_mode=TEST_MODE)

# æ˜¾ç¤ºç¤ºä¾‹
print(f"\nğŸ“„ Document ç¤ºä¾‹:")
print(f"   å†…å®¹: {documents[0].page_content[:80]}...")
print(f"   å…ƒæ•°æ®: {documents[0].metadata}")

# ==================== Step 3: æ„å»º LangChain Chroma å‘é‡åº“ ====================

print("\n" + "=" * 80)
print("Step 3: æ„å»º LangChain Chroma å‘é‡æ•°æ®åº“")
print("=" * 80)

# åˆ é™¤æ—§æ•°æ®åº“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
import shutil
if os.path.exists(CHROMA_PERSIST_DIR):
    shutil.rmtree(CHROMA_PERSIST_DIR)
    print(f"   â„¹ï¸ å·²åˆ é™¤æ—§æ•°æ®åº“: {CHROMA_PERSIST_DIR}")

# åˆ›å»º Chroma å‘é‡åº“ï¼ˆLangChain æ–¹å¼ï¼‰
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embeddings,
    persist_directory=CHROMA_PERSIST_DIR,
    collection_name=COLLECTION_NAME,
    collection_metadata={
        "description": "PHM ä¾å­˜è·¯å¾„æ¨¡å¼å‘é‡åº“ (LangChain)",
        "embedding_model": EMBEDDING_MODEL,
        "total_documents": len(documents),
        "created_at": datetime.now().isoformat()
    }
)

print(f"âœ… Chroma å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ")
print(f"   å­˜å‚¨è·¯å¾„: {CHROMA_PERSIST_DIR}")
print(f"   Collection: {COLLECTION_NAME}")
print(f"   æ–‡æ¡£æ•°é‡: {len(documents)}")

# æŒä¹…åŒ–åˆ°ç£ç›˜
vectorstore.persist()
print(f"âœ… æ•°æ®åº“å·²æŒä¹…åŒ–åˆ°ç£ç›˜")

# ==================== Step 4: åˆ›å»º LangChain Retriever ====================

print("\n" + "=" * 80)
print("Step 4: åˆ›å»º LangChain Retriever")
print("=" * 80)

# åˆ›å»ºæ£€ç´¢å™¨ï¼ˆæ”¯æŒå¤šç§æ£€ç´¢ç­–ç•¥ï¼‰
retriever = vectorstore.as_retriever(
    search_type="similarity",  # æˆ– "mmr"ï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰
    search_kwargs={
        "k": 5,  # è¿”å› Top-5 ç›¸ä¼¼æ–‡æ¡£
        # "filter": {"frequency": {"$gt": 5}},  # å¯é€‰ï¼šæŒ‰å…ƒæ•°æ®è¿‡æ»¤
    }
)

print(f"âœ… Retriever åˆ›å»ºå®Œæˆ")
print(f"   æ£€ç´¢ç­–ç•¥: similarity")
print(f"   Top-K: 5")

# ==================== Step 5: æµ‹è¯• LangChain RAG æ£€ç´¢ ====================

print("\n" + "=" * 80)
print("Step 5: æµ‹è¯• LangChain RAG æ£€ç´¢")
print("=" * 80)

test_queries = [
    "æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹è®¾å¤‡æ•…éšœ",
    "ä¼ æ„Ÿå™¨æ•°æ®ç”¨äºå¥åº·ç›‘æµ‹",
    "ç®—æ³•æå‡è¯Šæ–­å‡†ç¡®ç‡"
]

for query in test_queries:
    print(f"\n{'='*70}")
    print(f"ğŸ” æŸ¥è¯¢: {query}")
    print(f"{'='*70}")
    
    # ä½¿ç”¨ LangChain Retriever æ£€ç´¢
    retrieved_docs = retriever.get_relevant_documents(query)
    
    print(f"\nâœ… æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ¨¡å¼:")
    for i, doc in enumerate(retrieved_docs, 1):
        metadata = doc.metadata
        print(f"\n   [{i}] {metadata['semantic_pattern']}")
        print(f"       é¢‘æ¬¡: {metadata['frequency']}")
        print(f"       å¥æ³•è·¯å¾„: {metadata['syntactic_path'][:100]}...")
        
        # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå¦‚æœéœ€è¦ï¼‰
        # score = vectorstore.similarity_search_with_score(query, k=1)[0][1]
        # print(f"       ç›¸ä¼¼åº¦: {score:.4f}")

# ==================== Step 6: æ¼”ç¤ºå¦‚ä½•åŠ è½½å·²æœ‰æ•°æ®åº“ ====================

print("\n" + "=" * 80)
print("Step 6: æ¼”ç¤ºå¦‚ä½•åŠ è½½å·²æœ‰ LangChain å‘é‡æ•°æ®åº“")
print("=" * 80)

print(f"\nğŸ“ åœ¨å…¶ä»–è„šæœ¬ä¸­åŠ è½½å‘é‡åº“çš„ä»£ç :")
print(f"""
```python
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# 1. é‡æ–°åˆå§‹åŒ–ç›¸åŒçš„ Embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="{EMBEDDING_MODEL}",
    model_kwargs={{'device': '{EMBEDDING_DEVICE}'}},
    encode_kwargs={{'normalize_embeddings': True}}
)

# 2. åŠ è½½å·²æœ‰å‘é‡åº“
vectorstore = Chroma(
    persist_directory="{CHROMA_PERSIST_DIR}",
    embedding_function=embeddings,
    collection_name="{COLLECTION_NAME}"
)

# 3. åˆ›å»º Retriever
retriever = vectorstore.as_retriever(
    search_kwargs={{"k": 5}}
)

# 4. ä½¿ç”¨ Retriever
query = "ä½ çš„æŸ¥è¯¢æ–‡æœ¬"
docs = retriever.get_relevant_documents(query)

# 5. æå–æ¨¡å¼ç”¨äº Prompt
patterns = [doc.metadata['semantic_pattern'] for doc in docs]
```
""")

# ==================== æ€»ç»“ ====================

print("\n" + "=" * 80)
print("âœ… LangChain RAG å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
print("=" * 80)

print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
print(f"   - æ¨¡å¼æ€»æ•°: {len(documents)}")
print(f"   - å‘é‡ç»´åº¦: {len(test_embedding)}")
print(f"   - å­˜å‚¨è·¯å¾„: {CHROMA_PERSIST_DIR}")
print(f"   - Collection: {COLLECTION_NAME}")

print(f"\nğŸ¯ LangChain ä¼˜åŠ¿:")
print(f"   âœ… æ ‡å‡†åŒ–æ¥å£: ç¬¦åˆè®ºæ–‡å¼•ç”¨è§„èŒƒ")
print(f"   âœ… å¯æ›¿æ¢ç»„ä»¶: è½»æ¾åˆ‡æ¢ Embedding/VectorStore")
print(f"   âœ… é«˜çº§æ£€ç´¢: æ”¯æŒ MMRã€è¿‡æ»¤ã€é‡æ’åº")
print(f"   âœ… é›†æˆæ€§å¼º: ä¸ LangChain ç”Ÿæ€æ— ç¼å¯¹æ¥")

print(f"\nğŸ’¡ ä¸ä¹‹å‰æ–¹æ³•çš„åŒºåˆ«:")
print(f"   ä¹‹å‰: SentenceTransformer + ChromaDB åŸç”Ÿ API")
print(f"   ç°åœ¨: LangChain HuggingFaceEmbeddings + Chroma (LangChain)")
print(f"   ä¼˜åŠ¿: æ›´æ ‡å‡†ã€æ›´æ˜“ç»´æŠ¤ã€è®ºæ–‡å¯å¼•ç”¨ LangChain æ¡†æ¶")

print(f"\nğŸš€ ä¸‹ä¸€æ­¥:")
print(f"   1. é›†æˆåˆ°æŠ½å–è„šæœ¬ (exact_deepseek.py)")
print(f"   2. åœ¨ Prompt ä¸­æ³¨å…¥æ£€ç´¢åˆ°çš„æ¨¡å¼")
print(f"   3. å¯¹æ¯”æœ‰/æ—  RAG çš„æŠ½å–è´¨é‡")

print(f"\nç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
