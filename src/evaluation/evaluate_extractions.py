# -*- coding: utf-8 -*-
"""
å®ä½“å…³ç³»æŠ½å–ç»“æœè¯„ä¼°è„šæœ¬
ä½¿ç”¨ Gemini æ¨¡å‹å¯¹æŠ½å–ç»“æœè¿›è¡Œåˆç†æ€§è¯„ä¼°
"""
import os
import json
import time
import argparse
from pathlib import Path
from datetime import datetime, timezone
from tqdm import tqdm

# OpenAI SDK (Gemini å…¼å®¹æ¥å£)
from openai import OpenAI

# ------------------------------
# è·¯å¾„é…ç½®
# ------------------------------
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent

# è¯„ä¼° Prompt
EVAL_PROMPT_FILE = PROJECT_ROOT / "configs" / "prompts" / "prompt_eva.txt"

# å®éªŒé€‰æ‹©ï¼šå‘½ä»¤è¡Œä¼˜å…ˆï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡ï¼Œé»˜è®¤ exp02
_parser = argparse.ArgumentParser(add_help=False)
_parser.add_argument("--exp", dest="exp_id", choices=["exp02", "exp03"], default=None)
_parser.add_argument("--exp02", dest="exp02", action="store_true")
_parser.add_argument("--exp03", dest="exp03", action="store_true")
_parser.add_argument("--03", dest="exp03_short", action="store_true")
_parser.add_argument("--max", dest="max_items", type=int, default=None, help="ä»…è¯„ä¼°å‰ N æ¡ç»“æœ")
_args, _unknown = _parser.parse_known_args()
_env_exp = os.getenv("EXP_ID")
if _args.exp_id in {"exp02", "exp03"}:
    EXP_ID = _args.exp_id
elif _args.exp03 or _args.exp03_short:
    EXP_ID = "exp03"
elif _args.exp02:
    EXP_ID = "exp02"
elif _env_exp in {"exp02", "exp03"}:
    EXP_ID = _env_exp
else:
    EXP_ID = "exp02"

# å¯é€‰ï¼šé™åˆ¶è¯„ä¼°æ•°é‡ï¼ˆå‘½ä»¤è¡Œ --max æˆ–ç¯å¢ƒå˜é‡ MAX_EVALï¼‰
MAX_ITEMS = _args.max_items
if MAX_ITEMS is None:
    try:
        MAX_ITEMS = int(os.getenv("MAX_EVAL", "0")) or None
    except Exception:
        MAX_ITEMS = None

# ä¸‰ä¸ªæ¨¡å‹çš„æŠ½å–ç»“æœç›®å½•ï¼ˆæŒ‰å®éªŒåˆ†æµï¼‰
DEEPSEEK_DIR = Path(f"E:/langchain/outputs/extractions/{EXP_ID}/deepseek_rag")
GEMINI_DIR = Path(f"E:/langchain/outputs/extractions/{EXP_ID}/gemini_rag")
KIMI_DIR = Path(f"E:/langchain/outputs/extractions/{EXP_ID}/kimi_rag")

# åŸå§‹è®ºæ–‡ç›®å½•ï¼ˆç”¨äºæä¾›ä¸Šä¸‹æ–‡ï¼‰
PAPERS_DIR = PROJECT_ROOT / "data" / "raw" / "papers"

# è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•ï¼ˆæŒ‰å®éªŒåˆ†æµï¼‰
EVAL_OUTPUT_DIR = Path(f"E:/langchain/outputs/evaluations/{EXP_ID}")
EVAL_LOG_DIR = Path(f"E:/langchain/outputs/logs/{EXP_ID}/evaluation")

os.makedirs(EVAL_OUTPUT_DIR, exist_ok=True)
os.makedirs(EVAL_LOG_DIR, exist_ok=True)

# Gemini è¯„ä¼°é…ç½®
EVAL_MODEL = "gemini-2.5-pro"
PROVIDER_NAME = "gemini_evaluator"

# ------------------------------
# å·¥å…·å‡½æ•°
# ------------------------------
def now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()

def strip_code_fences(s: str) -> str:
    """å»é™¤ ```json ... ``` æ ·å¼çš„ä»£ç å›´æ """
    if not isinstance(s, str):
        return s
    s = s.strip()
    if s.startswith("```") and s.endswith("```"):
        s = s[3:-3].strip()
        if "\n" in s:
            first_line, rest = s.split("\n", 1)
            if first_line.strip().lower() in {"json", "js", "javascript"}:
                s = rest
    # å°è¯•æå– { } ä¹‹é—´çš„å†…å®¹
    first_brace = s.find('{')
    last_brace = s.rfind('}')
    if first_brace != -1 and last_brace != -1 and first_brace < last_brace:
        s = s[first_brace:last_brace + 1]
    return s

def parse_json_response(content: str) -> dict:
    """è§£æ JSON å“åº”,è‡ªåŠ¨æ¸…ç†ä»£ç å›´æ """
    if not content:
        raise ValueError("API è¿”å›äº†ç©ºå†…å®¹")
    
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        cleaned = strip_code_fences(content)
        try:
            return json.loads(cleaned)
        except json.JSONDecodeError as e:
            print(f"\nâŒ JSON è§£æå¤±è´¥: {e}")
            print(f"   åŸå§‹å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"   æ¸…ç†åå†…å®¹é•¿åº¦: {len(cleaned)} å­—ç¬¦")
            print(f"   æ¸…ç†åå†…å®¹é¢„è§ˆ: {cleaned[:500]}")
            raise ValueError(f"æ— æ³•è§£æ JSON: {e}")

# ------------------------------
# åˆå§‹åŒ– Gemini å®¢æˆ·ç«¯
# ------------------------------
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("è¯·è®¾ç½® GEMINI_API_KEY ç¯å¢ƒå˜é‡")

client = OpenAI(
    api_key=api_key,
    base_url="https://hiapi.online/v1"
)

# ------------------------------
# åŠ è½½è¯„ä¼° Prompt
# ------------------------------
print(f"ğŸ“„ åŠ è½½è¯„ä¼° Prompt: {EVAL_PROMPT_FILE}")
with open(EVAL_PROMPT_FILE, 'r', encoding='utf-8') as f:
    eval_prompt_template = f.read()

print(f"âœ… Prompt é•¿åº¦: {len(eval_prompt_template)} å­—ç¬¦\n")

# ------------------------------
# è¯„ä¼°å‡½æ•°
# ------------------------------
def evaluate_extraction(extraction_data: dict, paper_name: str, model_name: str) -> dict:
    """
    ä½¿ç”¨ Gemini è¯„ä¼°å•ä¸ªæŠ½å–ç»“æœ
    
    Args:
        extraction_data: æŠ½å–çš„å®ä½“å’Œå…³ç³» JSON
        paper_name: è®ºæ–‡åç§°
        model_name: æŠ½å–æ¨¡å‹åç§°
    
    Returns:
        è¯„ä¼°åçš„ JSON (æ·»åŠ äº† evaluation å­—æ®µ)
    """
    # æ„å»ºè¯„ä¼° Prompt
    extraction_json = json.dumps(extraction_data, ensure_ascii=False, indent=2)
    
    eval_prompt = eval_prompt_template + f"""

## å¾…è¯„ä¼°çš„æŠ½å–ç»“æœ

è®ºæ–‡: {paper_name}
æŠ½å–æ¨¡å‹: {model_name}

```json
{extraction_json}
```

è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¾“å‡ºè¯„ä¼°åçš„ JSON,ä¸ºæ¯ä¸ªå®ä½“å’Œå…³ç³»æ·»åŠ  `evaluation` å­—æ®µã€‚
"""
    
    # è°ƒç”¨ Gemini API
    try:
        response = client.chat.completions.create(
            model=EVAL_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ PHM é¢†åŸŸçš„çŸ¥è¯†æŠ½å–è¯„ä¼°ä¸“å®¶ã€‚åªè¾“å‡ºä¸¥æ ¼çš„ JSONï¼Œä¸æ·»åŠ ä»»ä½•è§£é‡Šã€‚"},
                {"role": "user", "content": eval_prompt}
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )
        
        content = response.choices[0].message.content
        evaluated_data = parse_json_response(content)
        
        # éªŒè¯è¿”å›æ ¼å¼
        if "entities" not in evaluated_data or "relations" not in evaluated_data:
            raise ValueError(f"è¿”å›çš„ JSON ç¼ºå°‘å¿…éœ€å­—æ®µ: {evaluated_data.keys()}")
        
        return {
            "evaluated_data": evaluated_data,
            "raw_response": content,
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            } if hasattr(response, 'usage') else None
        }
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        raise

# ------------------------------
# æ‰¹é‡è¯„ä¼°
# ------------------------------
def evaluate_model_results(model_name: str, extraction_dir: Path):
    """è¯„ä¼°å•ä¸ªæ¨¡å‹çš„æ‰€æœ‰æŠ½å–ç»“æœ"""
    
    print(f"\n{'='*80}")
    print(f"ğŸ” è¯„ä¼° {model_name} æ¨¡å‹çš„æŠ½å–ç»“æœ")
    print(f"{'='*80}")
    
    # è·å–æ‰€æœ‰ JSON æ–‡ä»¶
    json_files = sorted(extraction_dir.glob("*.json"))
    # è‹¥æŒ‡å®šæœ€å¤§è¯„ä¼°æ¡æ•°ï¼Œè¿›è¡Œè£å‰ª
    if MAX_ITEMS and MAX_ITEMS > 0:
        before = len(json_files)
        json_files = json_files[:MAX_ITEMS]
        print(f"ğŸ”¢ ä»…è¯„ä¼°å‰ {len(json_files)} æ¡ï¼ˆåŸ {before} æ¡ï¼‰")
    
    if not json_files:
        print(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½• JSON æ–‡ä»¶: {extraction_dir}")
        return
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(json_files)} ä¸ªæŠ½å–ç»“æœæ–‡ä»¶")
    
    # åˆ›å»ºæ¨¡å‹ä¸“ç”¨è¾“å‡ºç›®å½•ï¼ˆå¦‚ deepseek/gemini/kimiï¼‰
    model_eval_dir = EVAL_OUTPUT_DIR / model_name.lower()
    os.makedirs(model_eval_dir, exist_ok=True)
    
    # ç»Ÿè®¡ä¿¡æ¯
    success_count = 0
    failed_count = 0
    total_correct_entities = 0
    total_incorrect_entities = 0
    total_correct_relations = 0
    total_incorrect_relations = 0
    
    # è¯„ä¼°æ—¥å¿—
    eval_log = []
    
    # é€ä¸ªè¯„ä¼°
    for json_file in tqdm(json_files, desc=f"è¯„ä¼° {model_name}"):
        paper_name = json_file.stem
        
        try:
            # è¯»å–æŠ½å–ç»“æœ
            with open(json_file, 'r', encoding='utf-8') as f:
                extraction_data = json.load(f)
            
            # è°ƒç”¨è¯„ä¼°
            tqdm.write(f"   ğŸ“ è¯„ä¼°: {paper_name}")
            start_time = time.time()
            
            eval_result = evaluate_extraction(extraction_data, paper_name, model_name)
            
            eval_time = time.time() - start_time
            
            # åªä¿å­˜è¯„ä¼°åçš„ JSON ç»“æœ
            eval_output_file = model_eval_dir / f"{paper_name}_evaluated.json"
            with open(eval_output_file, 'w', encoding='utf-8') as f:
                json.dump(eval_result['evaluated_data'], f, ensure_ascii=False, indent=2)
            
            # ç»Ÿè®¡è¯„ä¼°ç»“æœ
            evaluated = eval_result['evaluated_data']
            entities_correct = sum(1 for e in evaluated.get('entities', []) if e.get('evaluation') == 'æ­£ç¡®')
            entities_incorrect = sum(1 for e in evaluated.get('entities', []) if e.get('evaluation') == 'é”™è¯¯')
            relations_correct = sum(1 for r in evaluated.get('relations', []) if r.get('evaluation') == 'æ­£ç¡®')
            relations_incorrect = sum(1 for r in evaluated.get('relations', []) if r.get('evaluation') == 'é”™è¯¯')
            
            total_correct_entities += entities_correct
            total_incorrect_entities += entities_incorrect
            total_correct_relations += relations_correct
            total_incorrect_relations += relations_incorrect
            
            # è®°å½•æ—¥å¿—
            log_entry = {
                "time": now_iso(),
                "paper": paper_name,
                "model": model_name,
                "status": "success",
                "eval_time": round(eval_time, 2),
                "entities": {
                    "total": len(evaluated.get('entities', [])),
                    "correct": entities_correct,
                    "incorrect": entities_incorrect,
                    "uncertain": len(evaluated.get('entities', [])) - entities_correct - entities_incorrect
                },
                "relations": {
                    "total": len(evaluated.get('relations', [])),
                    "correct": relations_correct,
                    "incorrect": relations_incorrect,
                    "uncertain": len(evaluated.get('relations', [])) - relations_correct - relations_incorrect
                },
                "usage": eval_result.get('usage'),
                "output_file": str(eval_output_file)
            }
            eval_log.append(log_entry)
            
            tqdm.write(f"   âœ… å®ä½“: {entities_correct}/{len(evaluated.get('entities', []))} æ­£ç¡®, "
                      f"å…³ç³»: {relations_correct}/{len(evaluated.get('relations', []))} æ­£ç¡®")
            
            success_count += 1
            
            # é¿å… API é™æµ
            time.sleep(1)
            
        except Exception as e:
            tqdm.write(f"   âŒ å¤±è´¥: {e}")
            
            # è®°å½•å¤±è´¥æ—¥å¿—
            log_entry = {
                "time": now_iso(),
                "paper": paper_name,
                "model": model_name,
                "status": "failed",
                "error": str(e)
            }
            eval_log.append(log_entry)
            
            failed_count += 1
            continue
    
    # ä¿å­˜è¯„ä¼°æ—¥å¿—
    log_file = EVAL_LOG_DIR / f"{model_name.lower()}_evaluation_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(eval_log, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°æ±‡æ€»
    print(f"\n{'='*80}")
    print(f"ğŸ“Š {model_name} è¯„ä¼°æ±‡æ€»")
    print(f"{'='*80}")
    print(f"âœ… æˆåŠŸ: {success_count}/{len(json_files)}")
    print(f"âŒ å¤±è´¥: {failed_count}/{len(json_files)}")
    
    if success_count > 0:
        total_entities = total_correct_entities + total_incorrect_entities
        total_relations = total_correct_relations + total_incorrect_relations
        
        entity_accuracy = (total_correct_entities / total_entities * 100) if total_entities > 0 else 0
        relation_accuracy = (total_correct_relations / total_relations * 100) if total_relations > 0 else 0
        
        print(f"\nå®ä½“å‡†ç¡®ç‡: {entity_accuracy:.2f}% ({total_correct_entities}/{total_entities})")
        print(f"å…³ç³»å‡†ç¡®ç‡: {relation_accuracy:.2f}% ({total_correct_relations}/{total_relations})")
    
    print(f"\nğŸ’¾ è¯„ä¼°æ—¥å¿—: {log_file}")
    print(f"ğŸ“ è¯„ä¼°ç»“æœ: {model_eval_dir}")
    
    return {
        "model": model_name,
        "success": success_count,
        "failed": failed_count,
        "total": len(json_files),
        "entity_accuracy": entity_accuracy if success_count > 0 else 0,
        "relation_accuracy": relation_accuracy if success_count > 0 else 0,
        "correct_entities": total_correct_entities,
        "incorrect_entities": total_incorrect_entities,
        "correct_relations": total_correct_relations,
        "incorrect_relations": total_incorrect_relations
    }

# ------------------------------
# ä¸»ç¨‹åº
# ------------------------------
def main():
    print("=" * 80)
    print("ğŸ”¬ å®éªŒç»“æœè´¨é‡è¯„ä¼°")
    print("=" * 80)
    print(f"è¯„ä¼°æ¨¡å‹: {EVAL_MODEL}")
    print(f"å®éªŒ: {EXP_ID}")
    print(f"æœ€å¤§è¯„ä¼°æ¡æ•°: {MAX_ITEMS if MAX_ITEMS else 'ä¸é™'}")
    print(f"è¯„ä¼° Prompt: {EVAL_PROMPT_FILE.name}")
    print(f"è¾“å‡ºç›®å½•: {EVAL_OUTPUT_DIR}")
    
    # è¯„ä¼°ä¸‰ä¸ªæ¨¡å‹
    models_to_evaluate = [
        ("DeepSeek", DEEPSEEK_DIR),
        ("Gemini", GEMINI_DIR),
        ("Kimi", KIMI_DIR)
    ]
    
    all_results = []
    
    for model_name, extraction_dir in models_to_evaluate:
        if not extraction_dir.exists():
            print(f"\nâš ï¸ è·³è¿‡ {model_name}: ç›®å½•ä¸å­˜åœ¨ ({extraction_dir})")
            continue
        
        result = evaluate_model_results(model_name, extraction_dir)
        all_results.append(result)
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    if all_results:
        print("\n" + "=" * 80)
        print("ğŸ“‹ æ¨¡å‹è¯„ä¼°å¯¹æ¯”æ±‡æ€»")
        print("=" * 80)
        
        import pandas as pd
        
        summary_df = pd.DataFrame([
            {
                "æ¨¡å‹": r["model"],
                "è¯„ä¼°æˆåŠŸæ•°": r["success"],
                "è¯„ä¼°å¤±è´¥æ•°": r["failed"],
                "å®ä½“å‡†ç¡®ç‡(%)": round(r["entity_accuracy"], 2),
                "å…³ç³»å‡†ç¡®ç‡(%)": round(r["relation_accuracy"], 2),
                "æ­£ç¡®å®ä½“æ•°": r["correct_entities"],
                "é”™è¯¯å®ä½“æ•°": r["incorrect_entities"],
                "æ­£ç¡®å…³ç³»æ•°": r["correct_relations"],
                "é”™è¯¯å…³ç³»æ•°": r["incorrect_relations"]
            }
            for r in all_results
        ])
        
        print(summary_df.to_string(index=False))
        
        # ä¿å­˜æ±‡æ€»
        summary_file = EVAL_OUTPUT_DIR / f"evaluation_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        summary_df.to_csv(summary_file, index=False, encoding='utf-8-sig')
        print(f"\nâœ… è¯„ä¼°æ±‡æ€»å·²ä¿å­˜: {summary_file}")
    
    print("\n" + "=" * 80)
    print("âœ… è¯„ä¼°å®Œæˆ!")
    print("=" * 80)

if __name__ == "__main__":
    main()
